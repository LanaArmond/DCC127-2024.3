{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importando Bibliotecas\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from AnalysisUtils import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn import cluster"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def encode_string_columns(df):\n",
    "  # Select columns of type 'object' or 'category'\n",
    "  non_numeric_columns = df.select_dtypes(include=['object', 'category'])\n",
    "\n",
    "  for col in non_numeric_columns.columns:\n",
    "    # Get the unique values in the column\n",
    "    unique_values = df[col].dropna().unique()\n",
    "    \n",
    "    if col == 'come_entre_refeicoes':\n",
    "      df['dummy_come_entre_refeicoes'] = df[col].str.lower().map({'nao':0, 'as vezes':1, 'frequentemente':2, 'sempre': 3})\n",
    "      continue\n",
    "      \n",
    "    if col == 'consumo_alcool':\n",
    "      df['dummy_consumo_alcool'] = df[col].str.lower().map({'nao':0, 'as vezes':1, 'frequentemente':2})\n",
    "      continue\n",
    "\n",
    "    if df[col].str.lower().isin(['sim', 'nao']).all():\n",
    "      df[f\"dummy_{col}\"] = df[col].str.lower().map({'sim': 1, 'nao': 0})\n",
    "      continue\n",
    "      \n",
    "    if len(unique_values) == 2:\n",
    "      label_encoder = LabelEncoder()\n",
    "      df[f\"dummy_{col}\"] = label_encoder.fit_transform(df[col])\n",
    "      continue\n",
    "    else:\n",
    "      df_dummies = pd.get_dummies(df[col], prefix=f\"dummy_{col}\")\n",
    "      df = pd.concat([df, df_dummies], axis=1)\n",
    "      continue\n",
    "        \n",
    "  return df\n",
    "\n",
    "def fill_missing_data(df, method='simple', n_neighbors=5):\n",
    "  if method == 'simple':\n",
    "    for col in df.columns:\n",
    "      if df[col].dtype in ['float64', 'int64']:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "      else:\n",
    "        if df[col].notna().any():\n",
    "          df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "  elif method == 'knn':\n",
    "    # Separate numeric and non-numeric columns\n",
    "    df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
    "    df_non_numeric = df.select_dtypes(exclude=['float64', 'int64'])\n",
    "\n",
    "    # Apply get_dummies for non-numeric columns to perform one-hot encoding\n",
    "    df_non_numeric_dummies = pd.get_dummies(df_non_numeric, drop_first=False)\n",
    "\n",
    "    # Impute using KNN\n",
    "    imputer_numeric = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputer_non_numeric = KNNImputer(n_neighbors=1)\n",
    "    \n",
    "    df_imputed_numeric = pd.DataFrame(\n",
    "      imputer_numeric.fit_transform(df_numeric),\n",
    "      columns=df_numeric.columns,\n",
    "      index=df_numeric.index\n",
    "    )\n",
    "\n",
    "    df_imputed_non_numeric = pd.DataFrame(\n",
    "      imputer_non_numeric.fit_transform(df_non_numeric_dummies),\n",
    "      columns=df_non_numeric_dummies.columns,\n",
    "      index=df_non_numeric_dummies.index\n",
    "    )\n",
    "    # Reverse the one-hot encoding by getting the most frequent category for each column\n",
    "    df_non_numeric_imputed = pd.DataFrame(index=df_imputed_non_numeric.index)\n",
    "    for col in df_non_numeric.columns:\n",
    "      vals_col = df_non_numeric[col].dropna().unique()\n",
    "      \n",
    "      dummies = [col + '_' + str(val) for val in vals_col]\n",
    "      dummies_values = df_imputed_non_numeric[dummies].idxmax(axis=1).apply(lambda x: x.split('_')[-1])\n",
    "      df_non_numeric_imputed[col] = dummies_values\n",
    "\n",
    "    # Combine numeric and non-numeric back to the original DataFrame\n",
    "    df = pd.concat([df_imputed_numeric, df_non_numeric_imputed], axis=1)\n",
    "\n",
    "  else:\n",
    "    raise ValueError(\"Invalid method. Choose from 'simple' or 'knn'.\")\n",
    "\n",
    "  return df\n",
    "\n",
    "def detect_outliers(df):\n",
    "  z_scores = np.abs(zscore(df.select_dtypes(include=[np.number])))\n",
    "  outliers = z_scores > 3  # Consider values with Z-score greater than 3 as outliers\n",
    "  return outliers.sum()\n",
    "\n",
    "def analyze_numerical_column(row):\n",
    "  print_simple_metrics(row)\n",
    "  boxplot_with_quartiles(row, yscale='linear')\n",
    "  create_filtered_histograms(row, log=False, filters=None, color='blue', bins=100)\n",
    "\n",
    "def calculate_group_metric(df, category_columns, numerical_columns, metric='median'):\n",
    "  result_list = []\n",
    "\n",
    "  for i in range(len(category_columns)):\n",
    "    for value in df[category_columns[i]].unique():\n",
    "      group_df = df[df[category_columns[i]] == value]\n",
    "\n",
    "      if metric == 'median':\n",
    "        group_metric = group_df[numerical_columns].median()\n",
    "      else:\n",
    "        group_metric = group_df[numerical_columns].mean()\n",
    "\n",
    "      group_metric['category'] = f\"{category_columns[i]}: {value}\"\n",
    "      result_list.append(group_metric)\n",
    "\n",
    "  result_df = pd.DataFrame(result_list)\n",
    "\n",
    "  return result_df\n",
    "\n",
    "def plot_sorted_group_metric(result_df, numerical_columns, metric='median'):\n",
    "  for col in numerical_columns:\n",
    "    # Sort the result DataFrame by the median of the current column\n",
    "    sorted_df = result_df.set_index('category').sort_values(by=col).reset_index()\n",
    "\n",
    "    # Create a new figure for each numerical column\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Prepare the boxplot data by creating a new DataFrame with the sorted categories\n",
    "    sorted_categories = sorted_df['category'].values\n",
    "    boxplot_data = []\n",
    "\n",
    "    for category in sorted_categories:\n",
    "      # Create the boxplot data by selecting the group corresponding to each category\n",
    "      group_category = category.split(\":\")[0]  # Extract the category name from 'column: value'\n",
    "      group_value = category.split(\":\")[1].strip()  # Extract the value\n",
    "      group_df = df[df[group_category] == group_value]  # Get the subgroup\n",
    "      boxplot_data.append(group_df[col].values)\n",
    "\n",
    "    # Create the boxplot, aligned by the sorted categories\n",
    "    sns.boxplot(data=boxplot_data, order=range(len(sorted_categories)))\n",
    "\n",
    "    # Set labels and title for the plot\n",
    "    plt.xticks(range(len(sorted_categories)), sorted_categories, rotation=45, ha='right')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel(f'{metric} of {col}')\n",
    "    plt.title(f'Sorted {metric}s of {col} by Category')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_category_columns(df, category_columns, color='skyblue', top=10):\n",
    "  num_columns = len(category_columns)\n",
    "  rows = (num_columns // 3) + (num_columns % 3 > 0)\n",
    "\n",
    "  fig, axes = plt.subplots(rows, 3, figsize=(15, 5 * rows))\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for idx, column in enumerate(category_columns):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Get the value counts for the encoded column\n",
    "    result_df = df[column].value_counts().reset_index()\n",
    "    result_df.columns = [column, 'Count']\n",
    "    result_df['Percentage'] = (result_df['Count'] / result_df['Count'].sum()) * 100\n",
    "\n",
    "    # Keep only top N values if specified\n",
    "    result_df = result_df.head(top)\n",
    "\n",
    "    # Plot the bar chart\n",
    "    result_df.plot(kind='bar', x=column, y='Count', color=color, ax=ax, legend=False)\n",
    "\n",
    "    ax.set_title(f\"Value Counts of {column}\", fontsize=12, pad=20)\n",
    "    ax.set_xlabel('Values', fontsize=10)\n",
    "    ax.set_ylabel('Count', fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for i, (count, pct) in enumerate(zip(result_df['Count'], result_df['Percentage'])):\n",
    "      ax.text(i, count + 0.5, f'{count} / {pct:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "  # Remove any empty axes\n",
    "  for i in range(idx + 1, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "  plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "def scale_data(df, columns):\n",
    "  scaler = StandardScaler()\n",
    "  df[columns] = scaler.fit_transform(df[columns])\n",
    "  return df\n",
    "\n",
    "def scale_and_prepare_data(df, categorical_columns, numerical_columns):\n",
    "  # One-hot encode categorical columns\n",
    "  df_dummies = pd.get_dummies(df[categorical_columns], drop_first=True)\n",
    "\n",
    "  # Initialize StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "\n",
    "  # Scale the specified numerical columns\n",
    "  df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "  # Scale the one-hot encoded categorical columns (df_dummies)\n",
    "  df_dummies_scaled = scaler.fit_transform(df_dummies)\n",
    "\n",
    "  # Convert the scaled dummies back to a DataFrame and assign correct column names manually\n",
    "  df_dummies_scaled = pd.DataFrame(df_dummies_scaled, columns=df_dummies.columns, index=df.index)\n",
    "\n",
    "  # Concatenate the scaled numerical columns with the scaled one-hot encoded columns\n",
    "  df = pd.concat([df[numerical_columns], df_dummies_scaled], axis=1)\n",
    "\n",
    "  return df\n",
    "\n",
    "def add_health_columns(df):\n",
    "    # Calcular IMC\n",
    "    df['IMC'] = df['peso'] / (df['altura'] ** 2)\n",
    "\n",
    "    # Classificar IMG\n",
    "    def classify_imc(img):\n",
    "        if img <= 18.5:\n",
    "            return 'Baixo'\n",
    "        elif img <= 24.9:\n",
    "            return 'Normal'\n",
    "        elif img <= 29.9:\n",
    "            return 'Sobrepeso'\n",
    "        else:\n",
    "            return 'Obesidade'\n",
    "\n",
    "    df['Class_IMC'] = df['IMC'].apply(classify_imc)\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_and_return_correlation(df, category_columns_dummies, numeric_columns, method='spearman', plot=True):\n",
    "\n",
    "  df_numeric = df[numeric_columns + category_columns_dummies]\n",
    "\n",
    "  correlation_matrix = df_numeric.corr(method=method)\n",
    "\n",
    "  # Plot the heatmap\n",
    "  if plot:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(f'Matriz de Correlação ({method.capitalize()})')\n",
    "    plt.show()\n",
    "\n",
    "  # Flatten the correlation matrix and sort by absolute value\n",
    "  corr_pairs = correlation_matrix.unstack()\n",
    "  sorted_corr = corr_pairs.sort_values(key=lambda x: x.abs(), ascending=False)\n",
    "\n",
    "  # Exclude self-correlations\n",
    "  filtered_corr = sorted_corr[sorted_corr.index.get_level_values(0) != sorted_corr.index.get_level_values(1)]\n",
    "\n",
    "  # Remove duplicates like [A, B] and [B, A]\n",
    "  unique_pairs = filtered_corr.reset_index()\n",
    "  unique_pairs['sorted_index'] = unique_pairs.apply(lambda row: tuple(sorted([row['level_0'], row['level_1']])), axis=1)\n",
    "  unique_pairs = unique_pairs.drop_duplicates(subset='sorted_index').drop(columns='sorted_index')\n",
    "\n",
    "  # Rename columns for clarity\n",
    "  unique_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "  return unique_pairs, correlation_matrix\n",
    "\n",
    "def show_changed_missing_values(df1, df2):\n",
    "\n",
    "  df1_ordered = df1[df2.columns]\n",
    "  changed = df1_ordered != df2\n",
    "  changed_rows_df1 = df1_ordered[changed].dropna(how='all')\n",
    "  changed_rows_df2 = df2[changed].dropna(how='all')\n",
    "\n",
    "  merged_values = changed_rows_df1.combine(\n",
    "    changed_rows_df2,\n",
    "    lambda x1, x2: pd.Series([f\"{v1} | {v2}\" if pd.notna(v1) and pd.notna(v2) else None for v1, v2 in zip(x1, x2)])\n",
    "  )\n",
    "\n",
    "  return merged_values\n",
    "\n",
    "def pca_analysis(df, columns, plot=True):\n",
    "  # Performing PCA\n",
    "  pca_df = df[columns].copy()\n",
    "  pca = PCA()\n",
    "  pca_data = pca.fit_transform(pca_df)\n",
    "  for i in range(pca_data.shape[1]):\n",
    "    pca_df[f'pca{i+1}'] = pca_data[:, i]\n",
    "\n",
    "  # Loading components\n",
    "  loadings = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=pca_df.columns[:pca.components_.shape[-1]],\n",
    "    index=[f'PCA{i + 1}' for i in range(pca.n_components_)]\n",
    "  )\n",
    "\n",
    "  # Explained variance ratio and PCA contributions\n",
    "  explained_variance_ratio = pca.explained_variance_ratio_\n",
    "  pca_contributions = (explained_variance_ratio[:5]).round(4)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pca_df['pca1'], pca_df['pca2'], s=50)\n",
    "    plt.title(\"PCA Clustering\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "  sorted_loadings = {}\n",
    "  for i in range(pca.n_components_):\n",
    "    pca_component = f'PCA{i + 1}'\n",
    "    top_contributors = loadings.iloc[i].abs().sort_values(ascending=False).head(5)\n",
    "\n",
    "    # Create a dictionary for the top contributors and their contributions\n",
    "    sorted_loadings[pca_component] = {\n",
    "      feature: top_contributors[feature] for feature in top_contributors.index\n",
    "    }\n",
    "\n",
    "  return pca_df, sorted_loadings, pca_contributions\n",
    "\n",
    "def pca_explained_variance_plot(df, columns):\n",
    "  pca_df = df[columns].copy()\n",
    "  pca = PCA()\n",
    "  pca_data = pca.fit_transform(pca_df)\n",
    "  explained_variance_ratio = pca.explained_variance_ratio_\n",
    "  cumulative_explained_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "  # Plot the cumulative explained variance vs the number of principal components\n",
    "  plt.figure(figsize=(8, 6))\n",
    "  plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='-')\n",
    "  plt.title(\"Cumulative Explained Variance vs. Number of Principal Components\")\n",
    "  plt.xlabel(\"Number of Principal Components\")\n",
    "  plt.ylabel(\"Cumulative Explained Variance (%)\")\n",
    "  plt.xticks(range(1, len(explained_variance_ratio) + 1))\n",
    "  plt.yticks([i / 10 for i in range(0, 11)])\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  # Now, let's calculate and plot the contribution of each original column to the total variance explained\n",
    "  loadings = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=pca_df.columns[:pca.components_.shape[-1]],\n",
    "    index=[f'PCA{i + 1}' for i in range(pca.n_components_)]\n",
    "  )\n",
    "  \n",
    "  # Calculate the contribution of each original feature across all principal components\n",
    "  feature_contributions = loadings.abs().sum(axis=0).sort_values(ascending=False)\n",
    "  \n",
    "  plt.figure(figsize=(12, 8))  # Set larger figure size\n",
    "  feature_contributions.plot(kind='bar', stacked=True, colormap='viridis', figsize=(12, 8))\n",
    "  plt.title(\"Feature Contribution to PCA Components\", fontsize=16, fontweight='bold')\n",
    "  plt.xlabel(\"Original Features\", fontsize=12)\n",
    "  plt.ylabel(\"Total Contribution to PCA Components\", fontsize=12)\n",
    "  plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "  plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  return cumulative_explained_variance, feature_contributions\n",
    "\n",
    "def kmeans_elbow(df, max_clusters):\n",
    "  # Lista para armazenar os valores de inertia\n",
    "  inertias = []\n",
    "\n",
    "  # Testar diferentes números de clusters (k)\n",
    "  k_values = range(1, max_clusters + 1)\n",
    "  for k in k_values:\n",
    "      kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "      kmeans.fit(df)\n",
    "      inertias.append(kmeans.inertia_)\n",
    "\n",
    "  # Plotar o gráfico do método do cotovelo\n",
    "  plt.figure(figsize=(8, 5))\n",
    "  plt.plot(k_values, inertias, 'o-', color='blue')\n",
    "  plt.xlabel('Número de clusters (k)')\n",
    "  plt.ylabel('Inércia')\n",
    "  plt.title('Método do Cotovelo')\n",
    "  plt.xticks(k_values)\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def silhouette_score_clusters_plot(df, max_clusters):\n",
    "  # Testar diferentes valores de k\n",
    "  best_clusters_amount = 0\n",
    "  max_silhouette_score = -1\n",
    "  silhouette_scores = []\n",
    "  k_values = range(2, max_clusters + 1)\n",
    "\n",
    "  for k in k_values:\n",
    "      kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "      labels = kmeans.fit_predict(df)\n",
    "      score = silhouette_score(df, labels)\n",
    "      silhouette_scores.append(score)\n",
    "      \n",
    "      if score > max_silhouette_score:\n",
    "        best_clusters_amount = k\n",
    "        max_silhouette_score = score\n",
    "\n",
    "  # Plotar o Silhouette Score\n",
    "  plt.figure(figsize=(8, 5))\n",
    "  plt.plot(k_values, silhouette_scores, 'o-', color='green')\n",
    "  plt.xticks(k_values)\n",
    "  plt.xlabel('Número de clusters (k)')\n",
    "  plt.ylabel('Coeficiente de Silhueta')\n",
    "  plt.title('Coeficiente de Silhueta para diferentes k')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  return best_clusters_amount, max_silhouette_score\n",
    "\n",
    "def davies_bouldin_index(df, max_clusters):\n",
    "  best_clusters_amount = 0\n",
    "  min_davies_bouldin_index = 99999999\n",
    "  db_scores = []\n",
    "  k_values = range(2, max_clusters + 1)\n",
    "\n",
    "  for k in k_values:\n",
    "      kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "      labels = kmeans.fit_predict(df)\n",
    "      score = davies_bouldin_score(df, labels)\n",
    "      db_scores.append(score)\n",
    "\n",
    "      if score < min_davies_bouldin_index:\n",
    "        min_davies_bouldin_index = score\n",
    "        best_clusters_amount = k\n",
    "\n",
    "  # Plotar Davies-Bouldin Index\n",
    "  plt.figure(figsize=(8, 5))\n",
    "  plt.plot(k_values, db_scores, 'o-', color='purple')\n",
    "  plt.xlabel('Número de clusters (k)')\n",
    "  plt.ylabel('Davies-Bouldin Index')\n",
    "  plt.title('Davies-Bouldin Index para diferentes k')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  return best_clusters_amount, min_davies_bouldin_index\n",
    "\n",
    "def kmeans_clusters_amount_analysis(df, max_clusters):\n",
    "  kmeans_elbow(df, max_clusters)\n",
    "  print(\"depois do elbow\")\n",
    "\n",
    "  best_clusters_amount_silhouette, max_silhouette_score = silhouette_score_clusters_plot(df, max_clusters)\n",
    "  kmeans_plot(df, best_clusters_amount_silhouette)\n",
    "\n",
    "  print(f'Melhor quantidade de clusters: {best_clusters_amount_silhouette} | Silhueta: {max_silhouette_score}')\n",
    "\n",
    "def plot_clusters_pca(df, labels, cluster_centers=None):\n",
    "  pca = PCA(n_components=2)\n",
    "  reduced_data = pca.fit_transform(df)\n",
    "\n",
    "  plt.figure(figsize=(8, 6))\n",
    "\n",
    "  unique_labels = np.unique(labels)\n",
    "  for label in unique_labels:\n",
    "    if label == -1:\n",
    "        color = 'black'\n",
    "        label_name = 'Noise'\n",
    "    else:\n",
    "        color = None\n",
    "        label_name = f'Cluster {label}'\n",
    "    \n",
    "    cluster_data = reduced_data[labels == label]\n",
    "    plt.scatter(cluster_data[:, 0], cluster_data[:, 1], label=label_name, c=color)\n",
    "  \n",
    "  if cluster_centers is not None:\n",
    "    centroids = pca.transform(cluster_centers)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='black', marker='x', label='Centroids')\n",
    "  \n",
    "  plt.title(\"Clusters visualizados com PCA\")\n",
    "  plt.xlabel(\"PCA Componente 1\")\n",
    "  plt.ylabel(\"PCA Componente 2\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def kmeans_plot(df, n_clusters, plot=True):\n",
    "  kmeans = cluster.KMeans(n_clusters=n_clusters)\n",
    "  y_kmeans = kmeans.fit_predict(df)\n",
    "\n",
    "  if not plot:\n",
    "    return y_kmeans\n",
    "  \n",
    "  plot_clusters_pca(df, kmeans.labels_, kmeans.cluster_centers_)\n",
    "\n",
    "def dbscan_analysis(df, eps, min_samples, plot=True):\n",
    "  dbscan = cluster.DBSCAN(eps=eps, min_samples=min_samples)\n",
    "  y_dbscan = dbscan.fit_predict(df)\n",
    "\n",
    "  if not plot:\n",
    "    return y_dbscan\n",
    "  \n",
    "  labels = dbscan.labels_\n",
    "  plot_clusters_pca(df, labels)  \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numerical_columns = ['idade', 'idade_int', 'altura', 'peso', 'consome_vegetais', 'consome_vegetais_int', 'n_refeicoes_int', 'consumo_diario_agua', 'consumo_diario_agua_int', 'frequencia_atividade_fisica', 'frequencia_atividade_fisica_int', 'tempo_usando_eletronicos', 'tempo_usando_eletronicos_int', 'IMC']\n",
    "\n",
    "category_columns = ['sexo', 'historico_obesidade_familia', 'consome_comida_calorica', 'come_entre_refeicoes', 'fuma', 'consumo_alcool', 'tipo_transporte', 'Class_IMC', 'tipo_transporte_adaptado']\n",
    "\n",
    "category_columns_dummies = [\n",
    "  'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_consumo_alcool',\n",
    "  'dummy_fuma',\n",
    "  'dummy_tipo_transporte_adaptado_ativo',\n",
    "  'dummy_tipo_transporte_adaptado_individual',\n",
    "  'dummy_tipo_transporte_adaptado_transporte publico'\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparação de estratégias de inputting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_test = pd.read_csv('data/trabalho2_dados_4.csv')\n",
    "missing_rows_idx = df_test[df_test.isna().any(axis=1)].index.tolist()\n",
    "\n",
    "df_test_knn = fill_missing_data(df_test, method='knn')\n",
    "df_test_simple = fill_missing_data(df_test, method='simple')\n",
    "\n",
    "show_changed_missing_values(df_test_knn, df_test_simple)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## buscando duplicada nova gerada com inputting (gerou para ambos)\n",
    "ele tinha transporte publico como NaN, mas ao substiruí-lo virou uma duplicada perfeita."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# analisando duplicata nova gerada pelo fill_missing_data\n",
    "df_test = pd.read_csv('data/trabalho2_dados_4.csv')\n",
    "\n",
    "print('Antes do Input')\n",
    "new_duplicate = df_test.loc[602].copy()\n",
    "display(df_test[df_test.eq(df_test.loc[1]).all(axis=1)])\n",
    "\n",
    "df_test = fill_missing_data(df_test) \n",
    "\n",
    "print('Depois do Input')\n",
    "display(df_test.loc[602])\n",
    "display(df_test[df_test.eq(df_test.loc[1]).all(axis=1)])\n",
    "new_duplicate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processando Dados"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importando dados\n",
    "df = pd.read_csv('data/trabalho2_dados_4.csv')\n",
    "df = fill_missing_data(df, method='knn') # substituindo valores faltantes numéricos pela media e categóricos pela moda\n",
    "\n",
    "\n",
    "df['idade_int'] = np.floor(df['idade'])\n",
    "df['n_refeicoes_int'] = df['n_refeicoes'].round()\n",
    "df['consome_vegetais_int'] = df['consome_vegetais'].round()\n",
    "df['consumo_diario_agua_int'] = df['consumo_diario_agua'].round()\n",
    "df['frequencia_atividade_fisica_int'] = df['frequencia_atividade_fisica'].round()\n",
    "df['tempo_usando_eletronicos_int'] = df['tempo_usando_eletronicos'].round()\n",
    "\n",
    "df['tipo_transporte_adaptado'] = df['tipo_transporte'].apply(\n",
    "  lambda x: 'ativo' if x in ['bicicleta', 'andando'] else 'automovel' if x in ['carro', 'moto', 'transporte publico'] else x)\n",
    "\n",
    "df = add_health_columns(df)\n",
    "df = encode_string_columns(df) # encodando colunas de string\n",
    "df = df.drop_duplicates(df, keep='first') # remoção de ~250 entradas duplicadas (substituição de valores faltantes e transformação direta em int nao criaram novas duplicados apenas 1 para o caso de substituição\n",
    "df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecção de outliers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "detect_outliers(df) \n",
    "# Não foram detectados outliers relevantes nas métricas numéricas\n",
    "# a unica metrica com outliers usando z_score foi a de idade, mas essa distribuição não é normal, e foram apenas 7 encontrados\n",
    "# nas métricas categóricas, isso removeria certas do categorias do dataset, portanto, não é necessário remover outliers neste dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análises simples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_all_category_columns(df, category_columns, color='skyblue', top=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for row in numerical_columns:\n",
    "  analyze_numerical_column(df[row]) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráficos categorias "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "result_df = calculate_group_metric(df, category_columns, numerical_columns, metric='mean')\n",
    "plot_sorted_group_metric(result_df, numerical_columns, metric='mean')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de correlação com spearmann"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sorted_corr, corr_matrix = plot_and_return_correlation(df, category_columns_dummies=category_columns_dummies, numeric_columns=['idade_int', 'altura', 'peso', 'consome_vegetais_int', 'n_refeicoes_int', 'consumo_diario_agua_int', 'frequencia_atividade_fisica_int', 'tempo_usando_eletronicos_int', 'IMC'], method='spearman')\n",
    "sorted_corr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analise de influencia de colunas int para correlação"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sorted_corr, corr_matrix = plot_and_return_correlation(df, category_columns_dummies=category_columns_dummies,\n",
    "                            numeric_columns=['idade_int', 'consome_vegetais_int', 'n_refeicoes_int',\n",
    "                                             'consumo_diario_agua_int', 'frequencia_atividade_fisica_int',\n",
    "                                             'tempo_usando_eletronicos_int'], method='spearman', plot=False)\n",
    "\n",
    "sorted_corr2, corr_matrix2 = plot_and_return_correlation(df, category_columns_dummies=category_columns_dummies,\n",
    "                                                       numeric_columns=['idade', 'consome_vegetais', 'n_refeicoes', 'consumo_diario_agua','frequencia_atividade_fisica', 'tempo_usando_eletronicos'],\n",
    "                                                       method='spearman', plot=False)\n",
    "\n",
    "\n",
    "diff = abs(corr_matrix.to_numpy()) - abs(corr_matrix2.to_numpy())\n",
    "mask1 = (abs(corr_matrix.to_numpy()) >= 0.3) & (diff > 0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(mask1, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(f'int vs sem int  (diff>0)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mask2 = (abs(corr_matrix.to_numpy()) >= 0.3) & (diff < 0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(mask2, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(f'int vs sem int (diff<0)')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# começando analises para agrupamento"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numerical_columns_clustering_refinado = [\n",
    "  'idade',\n",
    "  'consome_vegetais_int',\n",
    "  'n_refeicoes_int',\n",
    "  'consumo_diario_agua_int',\n",
    "  'frequencia_atividade_fisica_int',\n",
    "  'IMC'\n",
    "] \n",
    "numerical_columns_clustering = [\n",
    "  'idade',\n",
    "  'consome_vegetais',\n",
    "  'n_refeicoes',\n",
    "  'consumo_diario_agua',\n",
    "  'frequencia_atividade_fisica',\n",
    "  'tempo_usando_eletronicos',\n",
    "  'IMC',\n",
    "  'peso',\n",
    "  'altura'\n",
    "]\n",
    "\n",
    "category_columns_clustering_refinado = [\n",
    "  'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_consumo_alcool',\n",
    "  'dummy_fuma',\n",
    "  'dummy_tipo_transporte_andando',\n",
    "  'dummy_tipo_transporte_bicicleta',\n",
    "  'dummy_tipo_transporte_carro',\n",
    "  'dummy_tipo_transporte_moto',\n",
    "  'dummy_tipo_transporte_transporte publico',\n",
    "]\n",
    "\n",
    "category_columns_clustering = [\n",
    "  'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_consumo_alcool',\n",
    "  'dummy_fuma',\n",
    "  'dummy_tipo_transporte_andando', \n",
    "  'dummy_tipo_transporte_bicicleta',\n",
    "  'dummy_tipo_transporte_carro', \n",
    "  'dummy_tipo_transporte_moto',\n",
    "  'dummy_tipo_transporte_transporte publico',\n",
    "]\n",
    "\n",
    "print(f\"dimensionalidade: {len(numerical_columns_clustering + category_columns_clustering)}\")\n",
    "\n",
    "scaled_df = scale_data(df, list(set(numerical_columns_clustering + category_columns_clustering_refinado + numerical_columns_clustering_refinado + category_columns_clustering)))\n",
    "scaled_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_analysis_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_clustering_refinado + category_columns_clustering_refinado)\n",
    "\n",
    "print(f\"contribuiçoes de cada pca: {pca_contributions}\")\n",
    "loadings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_analysis_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_clustering + category_columns_clustering)\n",
    "\n",
    "print(f\"contribuiçoes de cada pca: {pca_contributions}\")\n",
    "loadings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Baseado no gráfico, podemos remover o último pca sem problemas.\")\n",
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(scaled_df, columns=numerical_columns_clustering_refinado + category_columns_clustering_refinado)\n",
    "\n",
    "\n",
    "print(\"Baseado no gráfico, podemos remover o último pca sem problemas.\")\n",
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(scaled_df, columns=numerical_columns_clustering + category_columns_clustering)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_clustering_refinado + category_columns_clustering_refinado, plot=False)\n",
    "\n",
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(df, columns=numerical_columns_clustering_refinado + category_columns_clustering_refinado)\n",
    "\n",
    "\n",
    "pca_df = pca_df[[f\"pca{i}\"for i in range(1, 15)]]\n",
    "pca_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "algoritmos selecionados:\n",
    "particionamento (kmeans)\n",
    "hierárquica (AGNES),\n",
    "densidade (optics)\n",
    "grade (clique)\n",
    "\n",
    "## Análise de estratégias\n",
    "idade > idade_int\n",
    "sem eletronicos > eletronicos\n",
    "IMC > PESO + ALTURA\n",
    "INT >= SEM INT (INT MELHORAR QUANDO < 10 GRUPOS, SEM INT MELHOR PARA > 10 GRUPOS)\n",
    "\n",
    "\n",
    "---------------\n",
    "consumo alcool frequentemente\n",
    "consome entre refeiçoes -> sempre e não\n",
    "consome comida calorica -> nao (analisar)\n",
    "\n",
    "\n",
    "consome vegetais > 1\n",
    "n_refeicoes == 3\n",
    "atividade fisica < 3"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analisando \"Genéricos\"\n",
    "> excelente para resultado removendo os \"diferentes\" vs \"genericos\"\n",
    "> \n",
    "> verificar grupos com homem vs mulher\n",
    "> \n",
    "> verificar por faixas etárias\n",
    "> \n",
    "> remover os que não consomem comida calorica atrapalhou o resultado\n",
    "> \n",
    "> int ajudou bastante no resultado\n",
    "> \n",
    "> remoção de subgrupos tratanto int como categoria ajudou muito o resultado\n",
    "> \n",
    "> remoção de tempo usando eletronicos ajudou bastante\n",
    "> \n",
    "> separação por sexo ajudou muito o resultado\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerical_columns_test = [\n",
    "  'idade',\n",
    "  'consome_vegetais_int',\n",
    "  'consumo_diario_agua_int',\n",
    "  'frequencia_atividade_fisica_int',\n",
    "  'IMC',\n",
    "]\n",
    "\n",
    "category_columns_test = [\n",
    "  'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_consumo_alcool',\n",
    "  'dummy_tipo_transporte',\n",
    "]\n",
    "\n",
    "test_df = df.copy()\n",
    "test_df = test_df[\n",
    "  (test_df['dummy_tipo_transporte_andando'] == 0) &\n",
    "  (test_df['dummy_tipo_transporte_bicicleta'] == 0) &\n",
    "  (test_df['dummy_tipo_transporte_moto'] == 0) &\n",
    "  (test_df['dummy_fuma'] == 0) &\n",
    "  (test_df['dummy_consumo_alcool'] < 2)\n",
    "  & (\n",
    "          (test_df['dummy_come_entre_refeicoes'] == 2)\n",
    "          | (test_df['dummy_come_entre_refeicoes'] == 1)\n",
    "  ) &\n",
    "  (test_df['consome_vegetais_int'] > 1) &\n",
    "  (test_df['n_refeicoes_int'] == 3)\n",
    "  & (test_df['frequencia_atividade_fisica_int'] < 3)\n",
    "  ]\n",
    "test_df['dummy_tipo_transporte'] = test_df['dummy_tipo_transporte_transporte publico']\n",
    "\n",
    "\n",
    "scaled_df = scale_data(test_df, list(set(numerical_columns_test + category_columns_test)))\n",
    "scaled_df = test_df[numerical_columns_test + category_columns_test]\n",
    "\n",
    "\n",
    "test_pca_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_test + category_columns_test, plot=False)\n",
    "\n",
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(scaled_df, columns=numerical_columns_test + category_columns_test)\n",
    "\n",
    "kmeans_clusters_amount_analysis(test_pca_df[[f\"pca{i}\"for i in range(1, 11)]], max_clusters=50)\n",
    "\n",
    "kmeans_clusters_amount_analysis(scaled_df, max_clusters=50)\n",
    "scaled_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analisando \"Genéricos\" mulheres"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numerical_columns_test = [\n",
    "  'idade',\n",
    "  'consome_vegetais_int',\n",
    "  'consumo_diario_agua_int',\n",
    "  'frequencia_atividade_fisica_int',\n",
    "  'IMC',\n",
    "]\n",
    "\n",
    "category_columns_test = [\n",
    "  # 'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_consumo_alcool',\n",
    "  'dummy_tipo_transporte',\n",
    "]\n",
    "\n",
    "test_df = df.copy()\n",
    "test_df = test_df[\n",
    "  (test_df['dummy_sexo'] == 0) & # mulher\n",
    "  (test_df['dummy_tipo_transporte_andando'] == 0) &\n",
    "  (test_df['dummy_tipo_transporte_bicicleta'] == 0) & \n",
    "  (test_df['dummy_tipo_transporte_moto'] == 0) & \n",
    "  (test_df['dummy_fuma'] == 0) &\n",
    "  (test_df['dummy_consumo_alcool'] < 2)\n",
    "  & (\n",
    "    (test_df['dummy_come_entre_refeicoes'] == 2)\n",
    "    | (test_df['dummy_come_entre_refeicoes'] == 1)\n",
    "  ) &\n",
    "  (test_df['consome_vegetais_int'] > 1) &\n",
    "  (test_df['n_refeicoes_int'] == 3)\n",
    "  & (test_df['frequencia_atividade_fisica_int'] < 3)\n",
    "]\n",
    "test_df['dummy_tipo_transporte'] = test_df['dummy_tipo_transporte_transporte publico']\n",
    "\n",
    "\n",
    "scaled_df = scale_data(test_df, list(set(numerical_columns_test + category_columns_test)))\n",
    "scaled_df = test_df[numerical_columns_test + category_columns_test]\n",
    "\n",
    "\n",
    "test_pca_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_test + category_columns_test, plot=False)\n",
    "\n",
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(scaled_df, columns=numerical_columns_test + category_columns_test)\n",
    "\n",
    "kmeans_clusters_amount_analysis(test_pca_df[[f\"pca{i}\"for i in range(1, 11)]], max_clusters=50)\n",
    "\n",
    "kmeans_clusters_amount_analysis(scaled_df, max_clusters=50)\n",
    "scaled_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analisando \"Genéricos\" homens"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerical_columns_test = [\n",
    "  'idade',\n",
    "  'consome_vegetais_int',\n",
    "  'consumo_diario_agua_int',\n",
    "  'frequencia_atividade_fisica_int',\n",
    "  'IMC',\n",
    "]\n",
    "\n",
    "category_columns_test = [\n",
    "  # 'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_consumo_alcool',\n",
    "  'dummy_tipo_transporte',\n",
    "]\n",
    "\n",
    "test_df = df.copy()\n",
    "test_df = test_df[\n",
    "  (test_df['dummy_sexo'] == 1) & #homem\n",
    "  (test_df['dummy_tipo_transporte_andando'] == 0) &\n",
    "  (test_df['dummy_tipo_transporte_bicicleta'] == 0) &\n",
    "  (test_df['dummy_tipo_transporte_moto'] == 0) &\n",
    "  (test_df['dummy_fuma'] == 0) &\n",
    "  (test_df['dummy_consumo_alcool'] < 2)\n",
    "  & (\n",
    "          (test_df['dummy_come_entre_refeicoes'] == 2)\n",
    "          | (test_df['dummy_come_entre_refeicoes'] == 1)\n",
    "  ) &\n",
    "  (test_df['consome_vegetais_int'] > 1) &\n",
    "  (test_df['n_refeicoes_int'] == 3)\n",
    "  & (test_df['frequencia_atividade_fisica_int'] < 3)\n",
    "  ]\n",
    "test_df['dummy_tipo_transporte'] = test_df['dummy_tipo_transporte_transporte publico']\n",
    "\n",
    "\n",
    "scaled_df = scale_data(test_df, list(set(numerical_columns_test + category_columns_test)))\n",
    "scaled_df = test_df[numerical_columns_test + category_columns_test]\n",
    "\n",
    "\n",
    "test_pca_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_test + category_columns_test, plot=False)\n",
    "\n",
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(scaled_df, columns=numerical_columns_test + category_columns_test)\n",
    "\n",
    "# kmeans_clusters_amount_analysis(test_pca_df[[f\"pca{i}\"for i in range(1, 9)]], max_clusters=50)\n",
    "\n",
    "kmeans_clusters_amount_analysis(scaled_df, max_clusters=50)\n",
    "scaled_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analisando \"diferentes\"\n",
    "> pouca diferenciação entre os transporte não usuais, mas incluilos na base atrapalhe muito\n",
    "> \n",
    "> remover os fumantes, ajuda mas não tanto quando transporte, porém é perceptível 2 subgrupos de fumantes\n",
    "> \n",
    "> consumo alto de alcool pareido com fuma porém não é tão diferente internamente\n",
    "> \n",
    "> pessoas com consumo entre refeiçoes não usuais atrapalham resultado, mas aparentem ter 2 subgrupos\n",
    "> \n",
    "> pessoas que comem poucos vegetais fazem muita diferença mas entre si não tem subgrupos\n",
    "> \n",
    "> atrapalha muito o resultado e pouquissima diferenciação interna para pessoas que não comem 3 refeiçoes por dia\n",
    "> \n",
    "> atrapalha muito o resulta e pouqiissima diferenciação interna para pessoas que realizam alta atividade fisica"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "  # pouca diferenciação entre os transporte não usuais, mas incluilos na base atrapalhe muito\n",
    "# remover os fumantes, ajuda mas não tanto quando transporte, porém é perceptível 2 subgrupos de fumantes\n",
    "# consumo alto de alcool pareido com fuma porém não é tão diferente internamente\n",
    "# pessoas com consumo entre refeiçoes não usuais atrapalham resultado, mas aparentem ter 2 subgrupos\n",
    "# pessoas que comem poucos vegetais fazem muita diferença mas entre si não tem subgrupos\n",
    "# atrapalha muito o resultado e pouquissima diferenciação interna para pessoas que não comem 3 refeiçoes por dia\n",
    "# atrapalha muito o resulta e pouqiissima diferenciação interna para pessoas que realizam alta atividade fisica\n",
    "\n",
    "numerical_columns_test = [\n",
    "  'idade',\n",
    "  'consome_vegetais_int',\n",
    "  'consumo_diario_agua_int',\n",
    "  'frequencia_atividade_fisica_int',\n",
    "  'IMC'\n",
    "]\n",
    "\n",
    "category_columns_test = [\n",
    "  'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_consumo_alcool',\n",
    "  'dummy_tipo_transporte',\n",
    "]\n",
    "\n",
    "\n",
    "test_df = df.copy()\n",
    "test_df = test_df[~(\n",
    "  # (test_df['dummy_tipo_transporte_andando'] == 0) &\n",
    "  # (test_df['dummy_tipo_transporte_bicicleta'] == 0) &\n",
    "  # (test_df['dummy_tipo_transporte_moto'] == 0) &\n",
    "  (test_df['dummy_fuma'] == 0)\n",
    "  # (test_df['dummy_consumo_alcool'] < 2)\n",
    "  # (\n",
    "  #         (test_df['dummy_come_entre_refeicoes'] == 2)\n",
    "  #         | (test_df['dummy_come_entre_refeicoes'] == 1)\n",
    "  # )\n",
    "  # (test_df['consome_vegetais_int'] > 1)\n",
    "  # (test_df['n_refeicoes_int'] == 3)\n",
    "  # (test_df['frequencia_atividade_fisica_int'] < 3)\n",
    ")]\n",
    "# test_df\n",
    "\n",
    "scaled_df = scale_data(test_df, list(set(numerical_columns_clustering + category_columns_clustering_refinado + numerical_columns_clustering_refinado + category_columns_clustering)))\n",
    "\n",
    "test1_df = scaled_df[category_columns_clustering_refinado + numerical_columns_test1]\n",
    "kmeans_clusters_amount_analysis(test1_df, max_clusters=5)\n",
    "test1_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "smokers_df = refined_df[refined_df['dummy_fuma'] > 0].drop(columns=['dummy_fuma'])\n",
    "not_smokers_df = refined_df[refined_df['dummy_fuma'] < 0].drop(columns=['dummy_fuma'])\n",
    "kmeans_clusters_amount_analysis(pca_df, max_clusters=10)\n",
    "kmeans_clusters_amount_analysis(not_refined_df, max_clusters=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
