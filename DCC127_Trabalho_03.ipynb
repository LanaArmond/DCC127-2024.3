{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importando Bibliotecas\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from AnalysisUtils import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn import cluster\n",
    "from sklearn.cluster import KMeans, OPTICS\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "from numpy.random import uniform\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import colormaps"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def encode_string_columns(df):\n",
    "  # Select columns of type 'object' or 'category'\n",
    "  non_numeric_columns = df.select_dtypes(include=['object', 'category'])\n",
    "\n",
    "  for col in non_numeric_columns.columns:\n",
    "    # Get the unique values in the column\n",
    "    unique_values = df[col].dropna().unique()\n",
    "    \n",
    "    if col == 'come_entre_refeicoes':\n",
    "      df['dummy_come_entre_refeicoes'] = df[col].str.lower().map({'nao':0, 'as vezes':1, 'frequentemente':2, 'sempre': 3})\n",
    "      continue\n",
    "      \n",
    "    if col == 'consumo_alcool':\n",
    "      df['dummy_consumo_alcool'] = df[col].str.lower().map({'nao':0, 'as vezes':1, 'frequentemente':2})\n",
    "      continue\n",
    "\n",
    "    if df[col].str.lower().isin(['sim', 'nao']).all():\n",
    "      df[f\"dummy_{col}\"] = df[col].str.lower().map({'sim': 1, 'nao': 0})\n",
    "      continue\n",
    "      \n",
    "    if len(unique_values) == 2:\n",
    "      label_encoder = LabelEncoder()\n",
    "      df[f\"dummy_{col}\"] = label_encoder.fit_transform(df[col])\n",
    "      continue\n",
    "    else:\n",
    "      df_dummies = pd.get_dummies(df[col], prefix=f\"dummy_{col}\")\n",
    "      df = pd.concat([df, df_dummies], axis=1)\n",
    "      continue\n",
    "        \n",
    "  return df\n",
    "\n",
    "def fill_missing_data(df, method='simple', n_neighbors=5):\n",
    "  if method == 'simple':\n",
    "    for col in df.columns:\n",
    "      if df[col].dtype in ['float64', 'int64']:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "      else:\n",
    "        if df[col].notna().any():\n",
    "          df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "  elif method == 'knn':\n",
    "    # Separate numeric and non-numeric columns\n",
    "    df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
    "    df_non_numeric = df.select_dtypes(exclude=['float64', 'int64'])\n",
    "\n",
    "    # Apply get_dummies for non-numeric columns to perform one-hot encoding\n",
    "    df_non_numeric_dummies = pd.get_dummies(df_non_numeric, drop_first=False)\n",
    "\n",
    "    # Impute using KNN\n",
    "    imputer_numeric = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputer_non_numeric = KNNImputer(n_neighbors=1)\n",
    "    \n",
    "    df_imputed_numeric = pd.DataFrame(\n",
    "      imputer_numeric.fit_transform(df_numeric),\n",
    "      columns=df_numeric.columns,\n",
    "      index=df_numeric.index\n",
    "    )\n",
    "\n",
    "    df_imputed_non_numeric = pd.DataFrame(\n",
    "      imputer_non_numeric.fit_transform(df_non_numeric_dummies),\n",
    "      columns=df_non_numeric_dummies.columns,\n",
    "      index=df_non_numeric_dummies.index\n",
    "    )\n",
    "    # Reverse the one-hot encoding by getting the most frequent category for each column\n",
    "    df_non_numeric_imputed = pd.DataFrame(index=df_imputed_non_numeric.index)\n",
    "    for col in df_non_numeric.columns:\n",
    "      vals_col = df_non_numeric[col].dropna().unique()\n",
    "      \n",
    "      dummies = [col + '_' + str(val) for val in vals_col]\n",
    "      dummies_values = df_imputed_non_numeric[dummies].idxmax(axis=1).apply(lambda x: x.split('_')[-1])\n",
    "      df_non_numeric_imputed[col] = dummies_values\n",
    "\n",
    "    # Combine numeric and non-numeric back to the original DataFrame\n",
    "    df = pd.concat([df_imputed_numeric, df_non_numeric_imputed], axis=1)\n",
    "\n",
    "  else:\n",
    "    raise ValueError(\"Invalid method. Choose from 'simple' or 'knn'.\")\n",
    "\n",
    "  return df\n",
    "\n",
    "def detect_outliers(df):\n",
    "  z_scores = np.abs(zscore(df.select_dtypes(include=[np.number])))\n",
    "  outliers = z_scores > 3  # Consider values with Z-score greater than 3 as outliers\n",
    "  return outliers.sum()\n",
    "\n",
    "def analyze_numerical_column(row):\n",
    "  print_simple_metrics(row)\n",
    "  boxplot_with_quartiles(row, yscale='linear')\n",
    "  create_filtered_histograms(row, log=False, filters=None, color='blue', bins=100)\n",
    "\n",
    "def calculate_group_metric(df, category_columns, numerical_columns, metric='median'):\n",
    "  result_list = []\n",
    "\n",
    "  for i in range(len(category_columns)):\n",
    "    for value in df[category_columns[i]].unique():\n",
    "      group_df = df[df[category_columns[i]] == value]\n",
    "\n",
    "      if metric == 'median':\n",
    "        group_metric = group_df[numerical_columns].median()\n",
    "      else:\n",
    "        group_metric = group_df[numerical_columns].mean()\n",
    "\n",
    "      group_metric['category'] = f\"{category_columns[i]}: {value}\"\n",
    "      result_list.append(group_metric)\n",
    "\n",
    "  result_df = pd.DataFrame(result_list)\n",
    "\n",
    "  return result_df\n",
    "\n",
    "def plot_sorted_group_metric(result_df, numerical_columns, metric='median'):\n",
    "  for col in numerical_columns:\n",
    "    # Sort the result DataFrame by the median of the current column\n",
    "    sorted_df = result_df.set_index('category').sort_values(by=col).reset_index()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    sorted_categories = sorted_df['category'].values\n",
    "    boxplot_data = []\n",
    "\n",
    "    for category in sorted_categories:\n",
    "      group_category = category.split(\":\")[0]  # Extract the category name from 'column: value'\n",
    "      group_value = category.split(\":\")[1].strip()  # Extract the value\n",
    "      group_df = df[df[group_category] == group_value]  # Get the subgroup\n",
    "      boxplot_data.append(group_df[col].values)\n",
    "\n",
    "    group_metrics = sorted_df[col].values\n",
    "    norm = Normalize(vmin=np.min(group_metrics), vmax=np.max(group_metrics))\n",
    "    colors = colormaps['viridis'](norm(group_metrics))\n",
    "    colors_list = colors.tolist()\n",
    "    print(len(boxplot_data), len(colors_list))\n",
    "\n",
    "    sns.boxplot(\n",
    "      data=boxplot_data,\n",
    "      order=range(len(sorted_categories)),\n",
    "      palette=colors_list\n",
    "    )\n",
    "    \n",
    "    plt.xticks(range(len(sorted_categories)), sorted_categories, rotation=45, ha='right')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel(f'{metric} of {col}')\n",
    "    plt.title(f'Sorted {metric}s of {col} by Category')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_category_columns(df, category_columns, color='skyblue', top=10):\n",
    "  num_columns = len(category_columns)\n",
    "  rows = (num_columns // 3) + (num_columns % 3 > 0)\n",
    "\n",
    "  fig, axes = plt.subplots(rows, 3, figsize=(15, 5 * rows))\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for idx, column in enumerate(category_columns):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Get the value counts for the encoded column\n",
    "    result_df = df[column].value_counts().reset_index()\n",
    "    result_df.columns = [column, 'Count']\n",
    "    result_df['Percentage'] = (result_df['Count'] / result_df['Count'].sum()) * 100\n",
    "\n",
    "    # Keep only top N values if specified\n",
    "    result_df = result_df.head(top)\n",
    "\n",
    "    # Plot the bar chart\n",
    "    result_df.plot(kind='bar', x=column, y='Count', color=color, ax=ax, legend=False)\n",
    "\n",
    "    ax.set_title(f\"Value Counts of {column}\", fontsize=12, pad=20)\n",
    "    ax.set_xlabel('Values', fontsize=10)\n",
    "    ax.set_ylabel('Count', fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for i, (count, pct) in enumerate(zip(result_df['Count'], result_df['Percentage'])):\n",
    "      ax.text(i, count + 0.5, f'{count} / {pct:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "  # Remove any empty axes\n",
    "  for i in range(idx + 1, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "  plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "def scale_data(df, columns):\n",
    "  scaler = StandardScaler()\n",
    "  new_df = df.copy()\n",
    "  new_df[columns] = scaler.fit_transform(df[columns])\n",
    "  return new_df\n",
    "\n",
    "def scale_and_prepare_data(df, categorical_columns, numerical_columns):\n",
    "  # One-hot encode categorical columns\n",
    "  df_dummies = pd.get_dummies(df[categorical_columns], drop_first=True)\n",
    "\n",
    "  # Initialize StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "\n",
    "  # Scale the specified numerical columns\n",
    "  df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "  # Scale the one-hot encoded categorical columns (df_dummies)\n",
    "  df_dummies_scaled = scaler.fit_transform(df_dummies)\n",
    "\n",
    "  # Convert the scaled dummies back to a DataFrame and assign correct column names manually\n",
    "  df_dummies_scaled = pd.DataFrame(df_dummies_scaled, columns=df_dummies.columns, index=df.index)\n",
    "\n",
    "  # Concatenate the scaled numerical columns with the scaled one-hot encoded columns\n",
    "  df = pd.concat([df[numerical_columns], df_dummies_scaled], axis=1)\n",
    "\n",
    "  return df\n",
    "\n",
    "def add_health_columns(df):\n",
    "    # Calcular IMC\n",
    "    df['IMC'] = df['peso'] / (df['altura'] ** 2)\n",
    "\n",
    "    # Classificar IMG\n",
    "    def classify_imc(img):\n",
    "        if img <= 18.5:\n",
    "            return 'Baixo'\n",
    "        elif img <= 24.9:\n",
    "            return 'Normal'\n",
    "        elif img <= 29.9:\n",
    "            return 'Sobrepeso'\n",
    "        else:\n",
    "            return 'Obesidade'\n",
    "\n",
    "    df['Class_IMC'] = df['IMC'].apply(classify_imc)\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_and_return_correlation(df, category_columns_dummies, numeric_columns, method='spearman', plot=True):\n",
    "\n",
    "  df_numeric = df[numeric_columns + category_columns_dummies]\n",
    "\n",
    "  correlation_matrix = df_numeric.corr(method=method)\n",
    "\n",
    "  # Plot the heatmap\n",
    "  if plot:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(f'Matriz de Correlação ({method.capitalize()})')\n",
    "    plt.show()\n",
    "\n",
    "  # Flatten the correlation matrix and sort by absolute value\n",
    "  corr_pairs = correlation_matrix.unstack()\n",
    "  sorted_corr = corr_pairs.sort_values(key=lambda x: x.abs(), ascending=False)\n",
    "\n",
    "  # Exclude self-correlations\n",
    "  filtered_corr = sorted_corr[sorted_corr.index.get_level_values(0) != sorted_corr.index.get_level_values(1)]\n",
    "\n",
    "  # Remove duplicates like [A, B] and [B, A]\n",
    "  unique_pairs = filtered_corr.reset_index()\n",
    "  unique_pairs['sorted_index'] = unique_pairs.apply(lambda row: tuple(sorted([row['level_0'], row['level_1']])), axis=1)\n",
    "  unique_pairs = unique_pairs.drop_duplicates(subset='sorted_index').drop(columns='sorted_index')\n",
    "\n",
    "  # Rename columns for clarity\n",
    "  unique_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "  return unique_pairs, correlation_matrix\n",
    "\n",
    "def show_changed_missing_values(df1, df2):\n",
    "\n",
    "  df1_ordered = df1[df2.columns]\n",
    "  changed = df1_ordered != df2\n",
    "  changed_rows_df1 = df1_ordered[changed].dropna(how='all')\n",
    "  changed_rows_df2 = df2[changed].dropna(how='all')\n",
    "\n",
    "  merged_values = changed_rows_df1.combine(\n",
    "    changed_rows_df2,\n",
    "    lambda x1, x2: pd.Series([f\"{v1} | {v2}\" if pd.notna(v1) and pd.notna(v2) else None for v1, v2 in zip(x1, x2)])\n",
    "  )\n",
    "\n",
    "  return merged_values\n",
    "\n",
    "def pca_analysis(df, columns, plot=True):\n",
    "  # Performing PCA\n",
    "  pca_df = df[columns].copy()\n",
    "  pca = PCA()\n",
    "  pca_data = pca.fit_transform(pca_df)\n",
    "  for i in range(pca_data.shape[1]):\n",
    "    pca_df[f'pca{i+1}'] = pca_data[:, i]\n",
    "\n",
    "  # Loading components\n",
    "  loadings = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=pca_df.columns[:pca.components_.shape[-1]],\n",
    "    index=[f'PCA{i + 1}' for i in range(pca.n_components_)]\n",
    "  )\n",
    "\n",
    "  # Explained variance ratio and PCA contributions\n",
    "  explained_variance_ratio = pca.explained_variance_ratio_\n",
    "  pca_contributions = (explained_variance_ratio[:5]).round(4)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pca_df['pca1'], pca_df['pca2'], s=50)\n",
    "    plt.title(\"PCA Clustering\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "  sorted_loadings = {}\n",
    "  for i in range(pca.n_components_):\n",
    "    pca_component = f'PCA{i + 1}'\n",
    "    top_contributors = loadings.iloc[i].abs().sort_values(ascending=False).head(5)\n",
    "\n",
    "    # Create a dictionary for the top contributors and their contributions\n",
    "    sorted_loadings[pca_component] = {\n",
    "      feature: top_contributors[feature] for feature in top_contributors.index\n",
    "    }\n",
    "\n",
    "  return pca_df, sorted_loadings, pca_contributions\n",
    "\n",
    "def pca_explained_variance_plot(df, columns):\n",
    "  pca_df = df[columns].copy()\n",
    "  pca = PCA()\n",
    "  pca_data = pca.fit_transform(pca_df)\n",
    "  explained_variance_ratio = pca.explained_variance_ratio_\n",
    "  cumulative_explained_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "  # Plot the cumulative explained variance vs the number of principal components\n",
    "  plt.figure(figsize=(8, 6))\n",
    "  plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='-')\n",
    "  plt.title(\"Cumulative Explained Variance vs. Number of Principal Components\")\n",
    "  plt.xlabel(\"Number of Principal Components\")\n",
    "  plt.ylabel(\"Cumulative Explained Variance (%)\")\n",
    "  plt.xticks(range(1, len(explained_variance_ratio) + 1))\n",
    "  plt.yticks([i / 10 for i in range(0, 11)])\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  # Now, let's calculate and plot the contribution of each original column to the total variance explained\n",
    "  loadings = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=pca_df.columns[:pca.components_.shape[-1]],\n",
    "    index=[f'PCA{i + 1}' for i in range(pca.n_components_)]\n",
    "  )\n",
    "  \n",
    "  # Calculate the contribution of each original feature across all principal components\n",
    "  feature_contributions = loadings.abs().sum(axis=0).sort_values(ascending=False)\n",
    "  \n",
    "  plt.figure(figsize=(12, 8))  # Set larger figure size\n",
    "  feature_contributions.plot(kind='bar', stacked=True, colormap='viridis', figsize=(12, 8))\n",
    "  plt.title(\"Feature Contribution to PCA Components\", fontsize=16, fontweight='bold')\n",
    "  plt.xlabel(\"Original Features\", fontsize=12)\n",
    "  plt.ylabel(\"Total Contribution to PCA Components\", fontsize=12)\n",
    "  plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "  plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  return cumulative_explained_variance, feature_contributions\n",
    "\n",
    "def kmeans_elbow(df, max_clusters):\n",
    "  # Lista para armazenar os valores de inertia\n",
    "  inertias = []\n",
    "\n",
    "  # Testar diferentes números de clusters (k)\n",
    "  k_values = range(1, max_clusters + 1)\n",
    "  for k in k_values:\n",
    "      kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "      kmeans.fit(df)\n",
    "      inertias.append(kmeans.inertia_)\n",
    "\n",
    "  # Plotar o gráfico do método do cotovelo\n",
    "  plt.figure(figsize=(8, 5))\n",
    "  plt.plot(k_values, inertias, 'o-', color='blue')\n",
    "  plt.xlabel('Número de clusters (k)')\n",
    "  plt.ylabel('Inércia')\n",
    "  plt.title('Método do Cotovelo')\n",
    "  plt.xticks(k_values)\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def silhouette_score_clusters_plot(df, max_clusters):\n",
    "  # Testar diferentes valores de k\n",
    "  best_clusters_amount = 0\n",
    "  max_silhouette_score = -1\n",
    "  silhouette_scores = []\n",
    "  k_values = range(2, max_clusters + 1)\n",
    "\n",
    "  for k in k_values:\n",
    "    avg_score = 0\n",
    "  \n",
    "    # Run KMeans 5 times for each k\n",
    "    for _ in range(5):\n",
    "      kmeans = KMeans(n_clusters=k, random_state=None)  # Ensure a different random state each run\n",
    "      labels = kmeans.fit_predict(df)\n",
    "      score = silhouette_score(df, labels)\n",
    "      avg_score += score\n",
    "  \n",
    "    # Calculate average score for this k\n",
    "    avg_score /= 5\n",
    "    silhouette_scores.append(avg_score)\n",
    "  \n",
    "    # Update best clusters amount if we have a new max silhouette score\n",
    "    if avg_score > max_silhouette_score:\n",
    "      best_clusters_amount = k\n",
    "      max_silhouette_score = avg_score\n",
    "\n",
    "  # Plotar o Silhouette Score\n",
    "  plt.figure(figsize=(8, 5))\n",
    "  plt.plot(k_values, silhouette_scores, 'o-', color='green')\n",
    "  plt.xticks(k_values)\n",
    "  plt.xlabel('Número de clusters (k)')\n",
    "  plt.ylabel('Coeficiente de Silhueta')\n",
    "  plt.title('Coeficiente de Silhueta para diferentes k')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  return best_clusters_amount, max_silhouette_score\n",
    "\n",
    "def davies_bouldin_index(df, max_clusters):\n",
    "  best_clusters_amount = 0\n",
    "  min_davies_bouldin_index = 99999999\n",
    "  db_scores = []\n",
    "  k_values = range(2, max_clusters + 1)\n",
    "\n",
    "  for k in k_values:\n",
    "      kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "      labels = kmeans.fit_predict(df)\n",
    "      score = davies_bouldin_score(df, labels)\n",
    "      db_scores.append(score)\n",
    "\n",
    "      if score < min_davies_bouldin_index:\n",
    "        min_davies_bouldin_index = score\n",
    "        best_clusters_amount = k\n",
    "\n",
    "  # Plotar Davies-Bouldin Index\n",
    "  plt.figure(figsize=(8, 5))\n",
    "  plt.plot(k_values, db_scores, 'o-', color='purple')\n",
    "  plt.xlabel('Número de clusters (k)')\n",
    "  plt.ylabel('Davies-Bouldin Index')\n",
    "  plt.title('Davies-Bouldin Index para diferentes k')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  return best_clusters_amount, min_davies_bouldin_index\n",
    "\n",
    "def kmeans_clusters_amount_analysis(df, max_clusters):\n",
    "  kmeans_elbow(df, max_clusters)\n",
    "  print(\"depois do elbow\")\n",
    "\n",
    "  best_clusters_amount_silhouette, max_silhouette_score = silhouette_score_clusters_plot(df, max_clusters)\n",
    "  kmeans_plot(df, best_clusters_amount_silhouette)\n",
    "\n",
    "  print(f'Melhor quantidade de clusters: {best_clusters_amount_silhouette} | Silhueta: {max_silhouette_score}')\n",
    "\n",
    "def plot_clusters_pca(df, labels, cluster_centers=None):\n",
    "  pca = PCA(n_components=2)\n",
    "  reduced_data = pca.fit_transform(df)\n",
    "\n",
    "  plt.figure(figsize=(8, 6))\n",
    "\n",
    "  unique_labels = np.unique(labels)\n",
    "  for label in unique_labels:\n",
    "    if label == -1:\n",
    "        color = 'black'\n",
    "        label_name = 'Noise'\n",
    "    else:\n",
    "        color = None\n",
    "        label_name = f'Cluster {label}'\n",
    "    \n",
    "    cluster_data = reduced_data[labels == label]\n",
    "    plt.scatter(cluster_data[:, 0], cluster_data[:, 1], label=label_name, c=color)\n",
    "  \n",
    "  if cluster_centers is not None:\n",
    "    centroids = pca.transform(cluster_centers)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='black', marker='x', label='Centroids')\n",
    "  \n",
    "  plt.title(\"Clusters visualizados com PCA\")\n",
    "  plt.xlabel(\"PCA Componente 1\")\n",
    "  plt.ylabel(\"PCA Componente 2\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def kmeans_plot(df, n_clusters, plot=True, random_state=None):\n",
    "  kmeans = cluster.KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "  y_kmeans = kmeans.fit_predict(df)\n",
    "\n",
    "  if plot:\n",
    "    plot_clusters_pca(df, kmeans.labels_, kmeans.cluster_centers_)\n",
    "      \n",
    "  return y_kmeans\n",
    "\n",
    "def dbscan_analysis(df, eps, min_samples, plot=True):\n",
    "  dbscan = cluster.DBSCAN(eps=eps, min_samples=min_samples)\n",
    "  y_dbscan = dbscan.fit_predict(df)\n",
    "\n",
    "  if not plot:\n",
    "    return y_dbscan\n",
    "  \n",
    "  labels = dbscan.labels_\n",
    "  plot_clusters_pca(df, labels)  \n",
    "\n",
    "def clustering_analysis(df, method, columns, max_clusters=50):\n",
    "  scaled_df = scale_data(df, list(set(columns)))\n",
    "  scaled_df = scaled_df[columns]\n",
    "  \n",
    "  if method == 'kmeans':\n",
    "    kmeans_clusters_amount_analysis(scaled_df, max_clusters=max_clusters)\n",
    "  elif method == 'optics':\n",
    "    optics_hyperparam_search(scaled_df)\n",
    "  elif method == 'hierarchy':\n",
    "    hierarchical_hyperparam_search(scaled_df)\n",
    "\n",
    "  pca_analysis(scaled_df, columns=columns, plot=False)\n",
    "  pca_explained_variance_plot(scaled_df, columns=columns)\n",
    "\n",
    "def clustering_method(df, method, columns, **kwargs):\n",
    "  results = {}\n",
    "  scaled_df = scale_data(df, list(set(columns)))\n",
    "  scaled_df = scaled_df[columns]\n",
    "\n",
    "  if method == 'kmeans':\n",
    "    n_clusters = kwargs['n_clusters']\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(scaled_df)\n",
    "    results['labels'] = kmeans.labels_\n",
    "    results['inertia'] = kmeans.inertia_\n",
    "\n",
    "  elif method == 'hierarchy':\n",
    "    linkage_method = kwargs['linkage_method']\n",
    "    diana_linkage = linkage(scaled_df, method=linkage_method)\n",
    "    results['linkage_matrix'] = diana_linkage\n",
    "\n",
    "  elif method == 'optics':\n",
    "    min_samples = kwargs['min_samples']\n",
    "    xi = kwargs['xi']\n",
    "    min_cluster_size = kwargs['min_cluster_size']\n",
    "    optics = OPTICS(min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size)\n",
    "    optics.fit(scaled_df)\n",
    "    results['labels'] = optics.labels_\n",
    "    results['reachability'] = optics.reachability_\n",
    "    results['core_distances'] = optics.core_distances_\n",
    "\n",
    "  else:\n",
    "    raise ValueError(f\"Method '{method}' is not supported. Choose from 'kmeans', 'diana', 'optics', or 'clique'.\")\n",
    "\n",
    "  return results\n",
    "\n",
    "def plot_best_silhouette(results_df):\n",
    "  # Group results by number of clusters and get the best silhouette score for each number of clusters\n",
    "  results_df['n_clusters'] = results_df['labels'].apply(lambda x: len(set(x)) - (1 if -1 in set(x) else 0))\n",
    "  best_results = results_df.loc[results_df.groupby('n_clusters')['silhouette_score'].idxmax()]\n",
    "\n",
    "  # Plot silhouette score vs number of clusters\n",
    "  plt.figure(figsize=(8, 6))\n",
    "  plt.plot(best_results['n_clusters'], best_results['silhouette_score'], marker='o', linestyle='-', color='b')\n",
    "  plt.title(\"Best Silhouette Score by Number of Clusters (OPTICS)\")\n",
    "  plt.xlabel(\"Number of Clusters\")\n",
    "  plt.ylabel(\"Silhouette Score\")\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def optics_hyperparam_search(df):\n",
    "  min_samples_range = [2, 3, 5, 5, 6, 7, 8, 9, 10]\n",
    "  xi_range = [0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4]\n",
    "  min_cluster_size_range = [0.001, 0.025, 0.05, 0.1, 0.15]\n",
    "\n",
    "  best_score = -2\n",
    "  best_params = {}\n",
    "\n",
    "  param_combinations = product(min_samples_range, xi_range, min_cluster_size_range)\n",
    "  results = []\n",
    "\n",
    "  for min_samples, xi, min_cluster_size in tqdm(param_combinations, desc=\"Hyperparameter search\", total=len(min_samples_range) * len(xi_range) * len(min_cluster_size_range)):\n",
    "    try:\n",
    "      optics = OPTICS(min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size)\n",
    "      optics.fit(df)\n",
    "      labels = optics.labels_\n",
    "\n",
    "      if len(set(labels)) <= 1 or (-1 in set(labels) and len(set(labels)) == 2):\n",
    "        continue\n",
    "\n",
    "      score = silhouette_score(df, labels)\n",
    "\n",
    "      results.append({\n",
    "        'min_samples': min_samples,\n",
    "        'xi': xi,\n",
    "        'min_cluster_size': min_cluster_size,\n",
    "        'silhouette_score': score,\n",
    "        'labels': labels\n",
    "      })\n",
    "\n",
    "      if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = {\n",
    "          'min_samples': min_samples,\n",
    "          'xi': xi,\n",
    "          'min_cluster_size': min_cluster_size,\n",
    "          'silhouette_score': score\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Skipping parameters (min_samples={min_samples}, xi={xi}, min_cluster_size={min_cluster_size}): {e}\")\n",
    "\n",
    "  # Plotting the results for best silhouette score\n",
    "  results_df = pd.DataFrame(results)\n",
    "  best_result = results_df[results_df['silhouette_score'] == best_score].iloc[0]\n",
    "\n",
    "  plot_best_silhouette(results_df)\n",
    "\n",
    "  # Retrain the model with best parameters\n",
    "  optics_best = OPTICS(min_samples=int(best_result['min_samples']),\n",
    "                       xi=best_result['xi'],\n",
    "                       min_cluster_size=best_result['min_cluster_size'])\n",
    "  optics_best.fit(df)\n",
    "  labels_best = optics_best.labels_\n",
    "\n",
    "  # Plot the clustering result using PCA\n",
    "  plot_clusters_pca(df, labels_best)\n",
    "  \n",
    "  print(best_params)\n",
    "  \n",
    "  return best_params\n",
    "\n",
    "def hierarchical_hyperparam_search(df):\n",
    "  # Define the hyperparameter ranges\n",
    "  linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "  thresholds = [0.5, 1, 1.5, 2, 2.5]  # Define the range of thresholds to search over\n",
    "\n",
    "  best_score = -2\n",
    "  best_params = {}\n",
    "  results = []\n",
    "\n",
    "  # Perform grid search over the hyperparameter combinations\n",
    "  param_combinations = product(linkage_methods, thresholds)\n",
    "  for linkage_method, threshold in tqdm(param_combinations, desc=\"Hyperparameter search\", total=len(linkage_methods) * len(thresholds)):\n",
    "    try:\n",
    "      # Perform hierarchical clustering using the current parameters\n",
    "      linkage_matrix = linkage(df, method=linkage_method)\n",
    "\n",
    "      # Generate flat clusters based on the threshold\n",
    "      labels = fcluster(linkage_matrix, t=threshold, criterion='distance')\n",
    "\n",
    "      if len(set(labels)) <= 1 or (-1 in set(labels) and len(set(labels)) == 2):\n",
    "        continue\n",
    "\n",
    "      # Compute silhouette score for the current clustering\n",
    "      score = silhouette_score(df, labels)\n",
    "\n",
    "      results.append({\n",
    "        'linkage_method': linkage_method,\n",
    "        'threshold': threshold,\n",
    "        'silhouette_score': score,\n",
    "        'labels': labels\n",
    "      })\n",
    "\n",
    "      # Update best parameters if current score is better\n",
    "      if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = {\n",
    "          'linkage_method': linkage_method,\n",
    "          'threshold': threshold,\n",
    "          'silhouette_score': score\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Skipping parameters (linkage_method={linkage_method}, threshold={threshold}): {e}\")\n",
    "\n",
    "  # Convert results to DataFrame\n",
    "  results_df = pd.DataFrame(results)\n",
    "\n",
    "  # Plot the best silhouette scores by number of clusters\n",
    "  plot_best_silhouette(results_df)\n",
    "\n",
    "  # Retrain the model with best parameters\n",
    "  best_result = results_df.loc[results_df['silhouette_score'].idxmax()]\n",
    "  best_linkage_method = best_result['linkage_method']\n",
    "  best_threshold = best_result['threshold']\n",
    "\n",
    "  # Retrain the hierarchical clustering model with the best parameters\n",
    "  linkage_matrix_best = linkage(df, method=best_linkage_method)\n",
    "  labels_best = fcluster(linkage_matrix_best, t=best_threshold, criterion='distance')\n",
    "\n",
    "  # Plot the clustering result using PCA\n",
    "  plot_clusters_pca(df, labels_best)\n",
    "\n",
    "  return best_params\n",
    "\n",
    "def load_and_preprocess_data(inputting_method):\n",
    "  df = pd.read_csv('data/trabalho2_dados_4.csv')\n",
    "  df = fill_missing_data(df, method=inputting_method) # substituindo valores faltantes numéricos pela media e categóricos pela moda\n",
    "  df['idade_int'] = np.floor(df['idade'])\n",
    "  df['n_refeicoes_int'] = df['n_refeicoes'].round()\n",
    "  df['consome_vegetais_int'] = df['consome_vegetais'].round()\n",
    "  df['consumo_diario_agua_int'] = df['consumo_diario_agua'].round()\n",
    "  df['frequencia_atividade_fisica_int'] = df['frequencia_atividade_fisica'].round()\n",
    "  df['tempo_usando_eletronicos_int'] = df['tempo_usando_eletronicos'].round()\n",
    "  df['tipo_transporte_adaptado'] = df['tipo_transporte'].apply(\n",
    "    lambda x: 'ativo' if x in ['bicicleta', 'andando'] else 'automovel' if x in ['carro', 'moto', 'transporte publico'] else x)\n",
    "  df = add_health_columns(df)\n",
    "  df = encode_string_columns(df) # encodando colunas de string\n",
    "  df = df.drop_duplicates(df, keep='first') # remoção de ~250 entradas duplicadas (substituição de valores faltantes e transformação direta em int nao criaram novas duplicados apenas 1 para o caso de substituição\n",
    "  return df\n",
    "\n",
    "def get_generic_df(df, separate_cols=False):\n",
    "  numerical_columns_test = [\n",
    "    'idade',\n",
    "    'consome_vegetais_int',\n",
    "    'consumo_diario_agua_int',\n",
    "    'frequencia_atividade_fisica_int',\n",
    "    'IMC',\n",
    "  ]\n",
    "  category_columns_test = [\n",
    "    'dummy_sexo',\n",
    "    'dummy_historico_obesidade_familia',\n",
    "    'dummy_consome_comida_calorica',\n",
    "    'dummy_come_entre_refeicoes',\n",
    "    'dummy_consumo_alcool',\n",
    "    'dummy_tipo_transporte_transporte publico',\n",
    "    'dummy_tipo_transporte_carro',\n",
    "  ]\n",
    "  \n",
    "  test_df = df.copy()\n",
    "  test_df = test_df[\n",
    "    (test_df['dummy_tipo_transporte_andando'] == 0) &\n",
    "    (test_df['dummy_tipo_transporte_bicicleta'] == 0) &\n",
    "    (test_df['dummy_tipo_transporte_moto'] == 0) &\n",
    "    (test_df['dummy_fuma'] == 0) &\n",
    "    (test_df['dummy_consumo_alcool'] < 2)\n",
    "    & (\n",
    "            (test_df['dummy_come_entre_refeicoes'] == 2)\n",
    "            | (test_df['dummy_come_entre_refeicoes'] == 1)\n",
    "    ) &\n",
    "    (test_df['consome_vegetais_int'] > 1) &\n",
    "    (test_df['n_refeicoes_int'] == 3)\n",
    "    & (test_df['frequencia_atividade_fisica_int'] < 3)\n",
    "    ]\n",
    "  test_df['dummy_tipo_transporte'] = test_df['dummy_tipo_transporte_transporte publico']\n",
    "  \n",
    "  if separate_cols:\n",
    "    return test_df.copy(), numerical_columns_test, category_columns_test\n",
    "  \n",
    "  return test_df.copy(), numerical_columns_test+category_columns_test\n",
    "\n",
    "def get_generic_women_df(df, separate_cols=False):\n",
    "  numerical_columns_test = [\n",
    "    'idade',\n",
    "    'consome_vegetais_int',\n",
    "    'consumo_diario_agua_int',\n",
    "    'frequencia_atividade_fisica_int',\n",
    "    'IMC',\n",
    "  ]\n",
    "\n",
    "  category_columns_test = [\n",
    "    # 'dummy_sexo',\n",
    "    'dummy_historico_obesidade_familia',\n",
    "    'dummy_consome_comida_calorica',\n",
    "    'dummy_come_entre_refeicoes',\n",
    "    'dummy_consumo_alcool',\n",
    "    'dummy_tipo_transporte_transporte publico',\n",
    "    'dummy_tipo_transporte_carro',\n",
    "  ]\n",
    "\n",
    "  test_df = df.copy()\n",
    "  test_df = test_df[\n",
    "    (test_df['dummy_sexo'] == 0) & # mulher\n",
    "    (test_df['dummy_tipo_transporte_andando'] == 0) &\n",
    "    (test_df['dummy_tipo_transporte_bicicleta'] == 0) &\n",
    "    (test_df['dummy_tipo_transporte_moto'] == 0) &\n",
    "    (test_df['dummy_fuma'] == 0) &\n",
    "    (test_df['dummy_consumo_alcool'] < 2)\n",
    "    & (\n",
    "            (test_df['dummy_come_entre_refeicoes'] == 2)\n",
    "            | (test_df['dummy_come_entre_refeicoes'] == 1)\n",
    "    ) &\n",
    "    (test_df['consome_vegetais_int'] > 1) &\n",
    "    (test_df['n_refeicoes_int'] == 3)\n",
    "    & (test_df['frequencia_atividade_fisica_int'] < 3)\n",
    "    ]\n",
    "  test_df['dummy_tipo_transporte'] = test_df['dummy_tipo_transporte_transporte publico']\n",
    "  \n",
    "  if separate_cols:\n",
    "    return test_df.copy(), numerical_columns_test, category_columns_test\n",
    "  \n",
    "  return test_df.copy(), numerical_columns_test + category_columns_test\n",
    "\n",
    "def get_generic_men_df(df, separate_cols=False):\n",
    "  numerical_columns_test = [\n",
    "    'idade',\n",
    "    'consome_vegetais_int',\n",
    "    'consumo_diario_agua_int',\n",
    "    'frequencia_atividade_fisica_int',\n",
    "    'IMC',\n",
    "  ]\n",
    "\n",
    "  category_columns_test = [\n",
    "    # 'dummy_sexo',\n",
    "    'dummy_historico_obesidade_familia',\n",
    "    'dummy_consome_comida_calorica',\n",
    "    'dummy_come_entre_refeicoes',\n",
    "    'dummy_consumo_alcool',\n",
    "    'dummy_tipo_transporte_transporte publico',\n",
    "    'dummy_tipo_transporte_carro',\n",
    "  ]\n",
    "\n",
    "  test_df = df.copy()\n",
    "  test_df = test_df[\n",
    "    (test_df['dummy_sexo'] == 1) & #homem\n",
    "    (test_df['dummy_tipo_transporte_andando'] == 0) &\n",
    "    (test_df['dummy_tipo_transporte_bicicleta'] == 0) &\n",
    "    (test_df['dummy_tipo_transporte_moto'] == 0) &\n",
    "    (test_df['dummy_fuma'] == 0) &\n",
    "    (test_df['dummy_consumo_alcool'] < 2)\n",
    "    & (\n",
    "            (test_df['dummy_come_entre_refeicoes'] == 2)\n",
    "            | (test_df['dummy_come_entre_refeicoes'] == 1)\n",
    "    ) &\n",
    "    (test_df['consome_vegetais_int'] > 1) &\n",
    "    (test_df['n_refeicoes_int'] == 3)\n",
    "    & (test_df['frequencia_atividade_fisica_int'] < 3)\n",
    "    ]\n",
    "  test_df['dummy_tipo_transporte'] = test_df['dummy_tipo_transporte_transporte publico']\n",
    "  \n",
    "  if separate_cols:\n",
    "    return test_df.copy(), numerical_columns_test, category_columns_test\n",
    "\n",
    "  return test_df.copy(), numerical_columns_test + category_columns_test\n",
    "\n",
    "def get_unusual_df(df, separate_cols=False):\n",
    "  numerical_columns_test = [\n",
    "    'idade',\n",
    "    'consome_vegetais_int',\n",
    "    'consumo_diario_agua_int',\n",
    "    'frequencia_atividade_fisica_int',\n",
    "    'IMC'\n",
    "  ]\n",
    "\n",
    "  category_columns_test = [\n",
    "    'dummy_sexo',\n",
    "    'dummy_historico_obesidade_familia',\n",
    "    'dummy_consome_comida_calorica',\n",
    "    'dummy_come_entre_refeicoes',\n",
    "    'dummy_consumo_alcool',\n",
    "    'dummy_tipo_transporte_transporte publico',\n",
    "    'dummy_tipo_transporte_carro',\n",
    "  ]\n",
    "\n",
    "\n",
    "  test_df = df.copy()\n",
    "  test_df = test_df[~(\n",
    "          (test_df['dummy_tipo_transporte_andando'] == 0) &\n",
    "          (test_df['dummy_tipo_transporte_bicicleta'] == 0) &\n",
    "          (test_df['dummy_tipo_transporte_moto'] == 0) &\n",
    "          (test_df['dummy_fuma'] == 0) &\n",
    "          (test_df['dummy_consumo_alcool'] < 2) &\n",
    "          (\n",
    "                  (test_df['dummy_come_entre_refeicoes'] == 2)\n",
    "                  | (test_df['dummy_come_entre_refeicoes'] == 1)\n",
    "          ) &\n",
    "          (test_df['consome_vegetais_int'] > 1) &\n",
    "          (test_df['n_refeicoes_int'] == 3) &\n",
    "          (test_df['frequencia_atividade_fisica_int'] < 3)\n",
    "  )]\n",
    "  if separate_cols:\n",
    "    return test_df.copy(), numerical_columns_test, category_columns_test\n",
    "  \n",
    "  return test_df.copy(), numerical_columns_test + category_columns_test\n",
    "\n",
    "def get_unrefined_df(df, separate_cols=False):\n",
    "  numerical_columns_test = [\n",
    "    'idade',\n",
    "    'consome_vegetais',\n",
    "    'n_refeicoes',\n",
    "    'consumo_diario_agua',\n",
    "    'frequencia_atividade_fisica',\n",
    "    'tempo_usando_eletronicos',\n",
    "    'IMC',\n",
    "    'peso',\n",
    "    'altura'\n",
    "  ]\n",
    "\n",
    "  category_columns_test = [\n",
    "    'dummy_sexo',\n",
    "    'dummy_historico_obesidade_familia',\n",
    "    'dummy_consome_comida_calorica',\n",
    "    'dummy_come_entre_refeicoes',\n",
    "    'dummy_consumo_alcool',\n",
    "    'dummy_fuma',\n",
    "    'dummy_tipo_transporte_andando',\n",
    "    'dummy_tipo_transporte_bicicleta',\n",
    "    'dummy_tipo_transporte_carro',\n",
    "    'dummy_tipo_transporte_moto',\n",
    "    'dummy_tipo_transporte_transporte publico',\n",
    "  ]\n",
    "  if separate_cols:\n",
    "    return df.copy(), numerical_columns_test, category_columns_test\n",
    "  \n",
    "  return df.copy(), numerical_columns_test + category_columns_test\n",
    "\n",
    "def hopkins_statistic(df, columns):\n",
    "  X = scale_data(df, list(set(columns)))\n",
    "  X = X[columns]\n",
    "  X=X.values  #convert dataframe to a numpy array\n",
    "  sample_size = int(X.shape[0]*0.4)\n",
    "\n",
    "\n",
    "  #a uniform random sample in the original data space\n",
    "  X_uniform_random_sample = uniform(X.min(axis=0), X.max(axis=0) ,(sample_size , X.shape[1]))\n",
    "\n",
    "  #a random sample of size sample_size from the original data X\n",
    "  random_indices=sample(range(0, X.shape[0], 1), sample_size)\n",
    "  X_sample = X[random_indices]\n",
    "\n",
    "  #initialise unsupervised learner for implementing neighbor searches\n",
    "  neigh = NearestNeighbors(n_neighbors=2)\n",
    "  nbrs=neigh.fit(X)\n",
    "\n",
    "  #u_distances = nearest neighbour distances from uniform random sample\n",
    "  u_distances , u_indices = nbrs.kneighbors(X_uniform_random_sample , n_neighbors=2)\n",
    "  u_distances = u_distances[: , 0] #distance to the first (nearest) neighbour\n",
    "\n",
    "  #w_distances = nearest neighbour distances from a sample of points from original data X\n",
    "  w_distances , w_indices = nbrs.kneighbors(X_sample , n_neighbors=2)\n",
    "  #distance to the second nearest neighbour (as the first neighbour will be the point itself, with distance = 0)\n",
    "  w_distances = w_distances[: , 1]\n",
    "\n",
    "  u_sum = np.sum(u_distances)\n",
    "  w_sum = np.sum(w_distances)\n",
    "\n",
    "  #compute and return hopkins' statistic\n",
    "  H = u_sum/ (u_sum + w_sum)\n",
    "  return H\n",
    "\n",
    "def clusters_analysis(df, numerical_columns, category_columns, n_clusters, random_state=0, metric='mean'):\n",
    "  cols = numerical_columns + category_columns\n",
    "  cluster_df = df.copy()\n",
    "  scaled_df = scale_data(df, list(set(cols)))\n",
    "  scaled_df = scaled_df[cols]\n",
    "  best_silhouette = -1\n",
    "  best_random_state = None\n",
    "  best_labels = None\n",
    "\n",
    "  for i in range(10):\n",
    "    current_random_state = random_state + i\n",
    "    labels = kmeans_plot(scaled_df, n_clusters, plot=False, random_state=current_random_state)\n",
    "    silhouette = silhouette_score(scaled_df, labels)\n",
    "\n",
    "    if silhouette > best_silhouette:\n",
    "      best_silhouette = silhouette\n",
    "      best_labels = labels\n",
    "      best_random_state = current_random_state\n",
    "\n",
    "  kmeans_plot(scaled_df, n_clusters, plot=True, random_state=best_random_state)\n",
    "  cluster_df['cluster'] = best_labels\n",
    "\n",
    "  print(f\" best silhouette: {best_silhouette}\")\n",
    "\n",
    "  cluster_numeric_metrics = calculate_cluster_metric(cluster_df, numerical_columns, metric=metric)\n",
    "  plot_stacked_bar_for_clusters(cluster_df, category_columns)\n",
    "  plot_sorted_cluster_metric(cluster_df, cluster_numeric_metrics, numerical_columns, metric=metric)\n",
    "\n",
    "  return cluster_df, find_discriminative_metrics(cluster_df, cols)\n",
    "\n",
    "def calculate_cluster_metric(df, numerical_columns, metric='mean'):\n",
    "  result_list = []\n",
    "\n",
    "  for cluster in df['cluster'].unique():\n",
    "    cluster_df = df[df['cluster'] == cluster]\n",
    "\n",
    "    if metric == 'mean':\n",
    "      cluster_metric = cluster_df[numerical_columns].mean()\n",
    "    elif metric == 'median':\n",
    "      cluster_metric = cluster_df[numerical_columns].median()\n",
    "    else:\n",
    "      raise ValueError(\"Invalid metric. Use 'mean' or 'median'.\")\n",
    "\n",
    "    cluster_metric['cluster'] = f\"Cluster {cluster}\"\n",
    "    result_list.append(cluster_metric)\n",
    "\n",
    "  result_df = pd.DataFrame(result_list)\n",
    "  return result_df\n",
    "\n",
    "def plot_sorted_cluster_metric(df, result_df, numerical_columns, metric='mean'):\n",
    "  for col in numerical_columns:\n",
    "    sorted_df = result_df.set_index('cluster').sort_values(by=col).reset_index()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    sorted_clusters = sorted_df['cluster'].values\n",
    "    boxplot_data = []\n",
    "    cluster_sizes = []\n",
    "    for cluster in sorted_clusters:\n",
    "      cluster_index = int(cluster.split(\" \")[1])\n",
    "      group_df = df[df['cluster'] == cluster_index]\n",
    "      boxplot_data.append(group_df[col].values)\n",
    "      cluster_sizes.append(len(group_df))  # Count the number of individuals in this cluster\n",
    "\n",
    "    cluster_means = sorted_df[col].values\n",
    "    norm = Normalize(vmin=np.min(cluster_means), vmax=np.max(cluster_means))\n",
    "    colors = colormaps['viridis'](norm(cluster_means))\n",
    "    colors_list = colors.tolist()\n",
    "\n",
    "    sns.boxplot(\n",
    "      data=boxplot_data,\n",
    "      order=range(len(sorted_clusters)),\n",
    "      palette=colors_list\n",
    "    )\n",
    "\n",
    "    cluster_labels_with_sizes = [\n",
    "      f\"{cluster} (n={size})\" for cluster, size in zip(sorted_clusters, cluster_sizes)\n",
    "    ]\n",
    "    plt.xticks(range(len(sorted_clusters)), cluster_labels_with_sizes, rotation=45, ha='right')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(f'{metric} of {col}')\n",
    "    plt.title(f'Sorted {metric}s of {col} by Cluster')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_stacked_bar_for_clusters(df, category_columns):\n",
    "  clusters = sorted(df['cluster'].unique())  # Sort clusters in ascending order\n",
    "\n",
    "  for category in category_columns:\n",
    "    category = category_dict[category]\n",
    "    category_counts = []\n",
    "\n",
    "    for cluster in clusters:\n",
    "      cluster_df = df[df['cluster'] == cluster]\n",
    "      category_count = cluster_df[category].value_counts(normalize=True) * 100\n",
    "      category_count = category_count.reindex(df[category].unique())\n",
    "      category_counts.append(category_count)\n",
    "\n",
    "    stacked_df = pd.DataFrame(category_counts, index=clusters)\n",
    "\n",
    "    ax = stacked_df.plot(kind='bar', stacked=True, figsize=(12, 8), colormap='viridis')\n",
    "\n",
    "    plt.title(f\"Stacked Bar Plot of Clusters by {category}\")\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def find_discriminative_metrics(df, cols):\n",
    "  results = []\n",
    "\n",
    "  for col in cols:\n",
    "    if col != 'cluster':\n",
    "      contingency_table = pd.crosstab(df['cluster'], df[col])\n",
    "      stat, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "      results.append({\n",
    "        \"column\": col,\n",
    "        \"statistic\": stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"test\": \"Chi-Square\"\n",
    "      })\n",
    "\n",
    "  results_df = pd.DataFrame(results).sort_values(\"p_value\")\n",
    "  return results_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numerical_columns = ['idade', 'idade_int', 'altura', 'peso', 'consome_vegetais', 'consome_vegetais_int', 'n_refeicoes_int', 'consumo_diario_agua', 'consumo_diario_agua_int', 'frequencia_atividade_fisica', 'frequencia_atividade_fisica_int', 'tempo_usando_eletronicos', 'tempo_usando_eletronicos_int', 'IMC']\n",
    "\n",
    "category_columns = ['sexo', 'historico_obesidade_familia', 'consome_comida_calorica', 'come_entre_refeicoes', 'fuma', 'consumo_alcool', 'tipo_transporte', 'Class_IMC', 'tipo_transporte_adaptado']\n",
    "\n",
    "category_columns_dummies = [\n",
    "  'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_consumo_alcool',\n",
    "  'dummy_fuma',\n",
    "  'dummy_tipo_transporte_adaptado',\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparação de estratégias de inputting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_test = pd.read_csv('data/trabalho2_dados_4.csv')\n",
    "missing_rows_idx = df_test[df_test.isna().any(axis=1)].index.tolist()\n",
    "\n",
    "df_test_knn = fill_missing_data(df_test, method='knn')\n",
    "df_test_simple = fill_missing_data(df_test, method='simple')\n",
    "\n",
    "show_changed_missing_values(df_test_knn, df_test_simple)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## buscando duplicada nova gerada com inputting (gerou para ambos)\n",
    "ele tinha transporte publico como NaN, mas ao substiruí-lo virou uma duplicada perfeita."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# analisando duplicata nova gerada pelo fill_missing_data\n",
    "df_test = pd.read_csv('data/trabalho2_dados_4.csv')\n",
    "\n",
    "print('Antes do Input')\n",
    "new_duplicate = df_test.loc[602].copy()\n",
    "display(df_test[df_test.eq(df_test.loc[1]).all(axis=1)])\n",
    "\n",
    "df_test = fill_missing_data(df_test) \n",
    "\n",
    "print('Depois do Input')\n",
    "display(df_test.loc[602])\n",
    "display(df_test[df_test.eq(df_test.loc[1]).all(axis=1)])\n",
    "new_duplicate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processando Dados"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importando dados\n",
    "df = load_and_preprocess_data(inputting_method='knn')\n",
    "simple_df = load_and_preprocess_data(inputting_method='simple')\n",
    "df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecção de outliers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "detect_outliers(df) \n",
    "# Não foram detectados outliers relevantes nas métricas numéricas\n",
    "# a unica metrica com outliers usando z_score foi a de idade, mas essa distribuição não é normal, e foram apenas 7 encontrados\n",
    "# nas métricas categóricas, isso removeria certas do categorias do dataset, portanto, não é necessário remover outliers neste dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análises simples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_all_category_columns(df, category_columns, color='skyblue', top=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for row in numerical_columns:\n",
    "  analyze_numerical_column(df[row]) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráficos categorias "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "result_df = calculate_group_metric(df, category_columns, numerical_columns, metric='mean')\n",
    "plot_sorted_group_metric(result_df, numerical_columns, metric='mean')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de correlação com spearmann"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sorted_corr, corr_matrix = plot_and_return_correlation(df, category_columns_dummies=category_columns_dummies, numeric_columns=['idade', 'altura', 'peso', 'consome_vegetais_int', 'n_refeicoes_int', 'consumo_diario_agua_int', 'frequencia_atividade_fisica_int', 'tempo_usando_eletronicos_int', 'IMC'], method='spearman')\n",
    "sorted_corr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analise de influencia de colunas int para correlação"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sorted_corr, corr_matrix = plot_and_return_correlation(df, category_columns_dummies=category_columns_dummies,\n",
    "                            numeric_columns=['idade', 'consome_vegetais_int', 'n_refeicoes_int',\n",
    "                                             'consumo_diario_agua_int', 'frequencia_atividade_fisica_int',\n",
    "                                             'tempo_usando_eletronicos_int'], method='spearman', plot=False)\n",
    "\n",
    "sorted_corr2, corr_matrix2 = plot_and_return_correlation(df, category_columns_dummies=category_columns_dummies,\n",
    "                                                       numeric_columns=['idade', 'consome_vegetais', 'n_refeicoes', 'consumo_diario_agua','frequencia_atividade_fisica', 'tempo_usando_eletronicos'],\n",
    "                                                       method='spearman', plot=False)\n",
    "\n",
    "labels = ['idade', 'consome_vegetais', 'n_refeicoes', 'consumo_diario_agua','frequencia_atividade_fisica', 'tempo_usando_eletronicos'] + category_columns_dummies\n",
    "diff = abs(corr_matrix.to_numpy()) - abs(corr_matrix2.to_numpy())\n",
    "mask1 = (abs(corr_matrix.to_numpy()) >= 0.15) & (diff > 0.03)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(diff, annot=True, cmap='coolwarm', fmt=\".2f\", xticklabels=labels, yticklabels=labels)\n",
    "plt.title(f'int vs sem int  (diff>0)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mask2 = (abs(corr_matrix.to_numpy()) >= 0.15) & (diff < 0.03)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(diff, annot=True, cmap='coolwarm', fmt=\".2f\", xticklabels=labels, yticklabels=labels)\n",
    "plt.title(f'int vs sem int (diff<0)')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# começando analises para agrupamento"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numerical_columns_clustering_refinado = [\n",
    "  'idade',\n",
    "  'consome_vegetais_int',\n",
    "  'n_refeicoes_int',\n",
    "  'consumo_diario_agua_int',\n",
    "  'frequencia_atividade_fisica_int',\n",
    "  'IMC'\n",
    "] \n",
    "numerical_columns_clustering = [\n",
    "  'idade',\n",
    "  'consome_vegetais',\n",
    "  'n_refeicoes',\n",
    "  'consumo_diario_agua',\n",
    "  'frequencia_atividade_fisica',\n",
    "  'tempo_usando_eletronicos',\n",
    "  'IMC',\n",
    "  'peso',\n",
    "  'altura'\n",
    "]\n",
    "category_columns_clustering_refinado = [\n",
    "  'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_consumo_alcool',\n",
    "  'dummy_fuma',\n",
    "  'dummy_tipo_transporte_andando',\n",
    "  'dummy_tipo_transporte_bicicleta',\n",
    "  'dummy_tipo_transporte_carro',\n",
    "  'dummy_tipo_transporte_moto',\n",
    "  'dummy_tipo_transporte_transporte publico',\n",
    "]\n",
    "category_columns_clustering = [\n",
    "  'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_consumo_alcool',\n",
    "  'dummy_fuma',\n",
    "  'dummy_tipo_transporte_andando', \n",
    "  'dummy_tipo_transporte_bicicleta',\n",
    "  'dummy_tipo_transporte_carro', \n",
    "  'dummy_tipo_transporte_moto',\n",
    "  'dummy_tipo_transporte_transporte publico',\n",
    "]\n",
    "\n",
    "print(f\"dimensionalidade: {len(numerical_columns_clustering + category_columns_clustering)}\")\n",
    "\n",
    "scaled_df = scale_data(df, list(set(numerical_columns_clustering + category_columns_clustering_refinado + numerical_columns_clustering_refinado + category_columns_clustering)))\n",
    "scaled_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_analysis_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_clustering_refinado + category_columns_clustering_refinado)\n",
    "\n",
    "print(f\"contribuiçoes de cada pca: {pca_contributions}\")\n",
    "loadings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_analysis_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_clustering + category_columns_clustering)\n",
    "\n",
    "print(f\"contribuiçoes de cada pca: {pca_contributions}\")\n",
    "loadings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Baseado no gráfico, podemos remover o último pca sem problemas.\")\n",
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(scaled_df, columns=numerical_columns_clustering_refinado + category_columns_clustering_refinado)\n",
    "\n",
    "\n",
    "print(\"Baseado no gráfico, podemos remover o último pca sem problemas.\")\n",
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(scaled_df, columns=numerical_columns_clustering + category_columns_clustering)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_clustering_refinado + category_columns_clustering_refinado, plot=False)\n",
    "\n",
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(df, columns=numerical_columns_clustering_refinado + category_columns_clustering_refinado)\n",
    "\n",
    "\n",
    "pca_df = pca_df[[f\"pca{i}\"for i in range(1, 15)]]\n",
    "pca_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "algoritmos selecionados:\n",
    "particionamento (kmeans)\n",
    "hierárquica (AGNES),\n",
    "densidade (optics)\n",
    "grade (clique)\n",
    "\n",
    "## Análise de estratégias\n",
    "idade > idade_int\n",
    "sem eletronicos > eletronicos\n",
    "IMC > PESO + ALTURA\n",
    "INT >= SEM INT (INT MELHORAR QUANDO < 10 GRUPOS, SEM INT MELHOR PARA > 10 GRUPOS)\n",
    "\n",
    "\n",
    "---------------\n",
    "consumo alcool frequentemente\n",
    "consome entre refeiçoes -> sempre e não\n",
    "consome comida calorica -> nao (analisar)\n",
    "\n",
    "\n",
    "consome vegetais > 1\n",
    "n_refeicoes == 3\n",
    "atividade fisica < 3"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analisando \"Genéricos\"\n",
    "> excelente para resultado removendo os \"diferentes\" vs \"genericos\"\n",
    "> \n",
    "> verificar grupos com homem vs mulher\n",
    "> \n",
    "> verificar por faixas etárias\n",
    "> \n",
    "> remover os que não consomem comida calorica atrapalhou o resultado\n",
    "> \n",
    "> int ajudou bastante no resultado\n",
    "> \n",
    "> remoção de subgrupos tratanto int como categoria ajudou muito o resultado\n",
    "> \n",
    "> remoção de tempo usando eletronicos ajudou bastante\n",
    "> \n",
    "> separação por sexo ajudou muito o resultado\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "generic_df, cols = get_generic_df(df)\n",
    "generic_simple_df, _ = get_generic_df(simple_df)\n",
    "clustering_analysis(generic_df, method='kmeans', columns=cols)\n",
    "clustering_analysis(generic_simple_df, method='kmeans', columns=cols)\n",
    "# clustering_analysis(generic_df, method='optics', columns=cols)\n",
    "# clustering_analysis(generic_df, method='hierarchy', columns=cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analisando \"Genéricos\" mulheres"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "generic_women_df, cols = get_generic_women_df(df)\n",
    "generic_women_simple_df, _ = get_generic_women_df(simple_df)\n",
    "clustering_analysis(generic_women_df, method='kmeans', columns=cols)\n",
    "clustering_analysis(generic_women_simple_df, method='kmeans', columns=cols)\n",
    "# clustering_analysis(generic_women_df, method='optics', columns=cols)\n",
    "# clustering_analysis(generic_women_df, method='hierarchy', columns=cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analisando \"Genéricos\" homens"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "generic_men_df, cols = get_generic_men_df(df)\n",
    "generic_men_simple_df, _ = get_generic_men_df(simple_df)\n",
    "clustering_analysis(generic_men_df, method='kmeans', columns=cols)\n",
    "clustering_analysis(generic_men_simple_df, method='kmeans', columns=cols)\n",
    "# clustering_analysis(generic_men_df, method='optics', columns=cols)\n",
    "# clustering_analysis(generic_men_df, method='hierarchy', columns=cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analisando \"diferentes\"\n",
    "> pouca diferenciação entre os transporte não usuais, mas incluilos na base atrapalhe muito\n",
    "> \n",
    "> remover os fumantes, ajuda mas não tanto quando transporte, porém é perceptível 2 subgrupos de fumantes\n",
    "> \n",
    "> consumo alto de alcool pareido com fuma porém não é tão diferente internamente\n",
    "> \n",
    "> pessoas com consumo entre refeiçoes não usuais atrapalham resultado, mas aparentem ter 2 subgrupos\n",
    "> \n",
    "> pessoas que comem poucos vegetais fazem muita diferença mas entre si não tem subgrupos\n",
    "> \n",
    "> atrapalha muito o resultado e pouquissima diferenciação interna para pessoas que não comem 3 refeiçoes por dia\n",
    "> \n",
    "> atrapalha muito o resulta e pouqiissima diferenciação interna para pessoas que realizam alta atividade fisica"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unusual_df, cols = get_unusual_df(df)\n",
    "unusual_simple_df, _ = get_unusual_df(simple_df)\n",
    "clustering_analysis(unusual_df, method='kmeans', columns=cols)\n",
    "clustering_analysis(unusual_simple_df, method='kmeans', columns=cols)\n",
    "# clustering_analysis(unusual_df, method='optics', columns=cols)\n",
    "# clustering_analysis(unusual_df, method='hierarchy', columns=cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unrefined_df, cols = get_unrefined_df(df)\n",
    "unrefined_simple_df, _ = get_unrefined_df(simple_df)\n",
    "clustering_analysis(unrefined_df, method='kmeans', columns=cols)\n",
    "clustering_analysis(unrefined_simple_df, method='kmeans', columns=cols)\n",
    "# clustering_analysis(unrefined_df, method='optics', columns=cols)\n",
    "# clustering_analysis(unrefined_df, method='hierarchy', columns=cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "category_dict = {\n",
    "  'dummy_sexo':'sexo',\n",
    "  'dummy_historico_obesidade_familia':'historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica':'consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes':'come_entre_refeicoes',\n",
    "  'dummy_consumo_alcool':'fuma',\n",
    "  'dummy_fuma':'consumo_alcool',\n",
    "  'dummy_tipo_transporte_transporte publico': 'tipo_transporte',\n",
    "  'dummy_tipo_transporte_carro': 'tipo_transporte',\n",
    "  'dummy_tipo_transporte_andando': 'tipo_transporte',\n",
    "  'dummy_tipo_transporte_bicicleta': 'tipo_transporte',\n",
    "  'dummy_tipo_transporte_moto': 'tipo_transporte',\n",
    "  'dummy_tipo_transporte_adaptado':'tipo_transporte_adaptado'\n",
    "}\n",
    "\n",
    "generic_df, generic_numeric_cols, generic_category_cols = get_generic_df(df, separate_cols=True)\n",
    "unrefined_df, unrefined_numeric_cols, unrefined_category_cols = get_unrefined_df(df, separate_cols=True)\n",
    "unusual_df, unusual_numeric_cols, unusual_category_cols = get_unusual_df(df, separate_cols=True)\n",
    "generic_men_df, generic_men_numeric_cols, generic_men_category_cols = get_generic_men_df(df, separate_cols=True)\n",
    "generic_women_df, generic_women_numeric_cols, generic_women_category_cols = get_generic_women_df(df, separate_cols=True)\n",
    "\n",
    "print(f\"Hopkins test for generic_df, with value: {hopkins_statistic(generic_df, columns=generic_numeric_cols + generic_category_cols)}\")\n",
    "print(f\"Hopkins test for unrefined_df, with value: {hopkins_statistic(unrefined_df, columns=unrefined_numeric_cols + unrefined_category_cols)}\")\n",
    "print(f\"Hopkins test for unusual_df, with value: {hopkins_statistic(unusual_df, columns=unusual_numeric_cols + unusual_category_cols)}\")\n",
    "print(f\"Hopkins test for generic_men_df, with value: {hopkins_statistic(generic_men_df, columns=generic_men_numeric_cols +  generic_men_category_cols)}\")\n",
    "print(f\"Hopkins test for generic_women_df, with value: {hopkins_statistic(generic_women_df, columns=generic_women_numeric_cols + generic_women_category_cols)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cluster_df, discriminative_metrics = clusters_analysis(generic_df, n_clusters=8, numerical_columns=generic_numeric_cols, category_columns=generic_category_cols)\n",
    "discriminative_metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cluster_df, discriminative_metrics = clusters_analysis(generic_women_df, n_clusters=3, numerical_columns=generic_women_numeric_cols, category_columns=generic_women_category_cols)\n",
    "discriminative_metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cluster_df, discriminative_metrics = clusters_analysis(generic_men_df, n_clusters=3, numerical_columns=generic_men_numeric_cols, category_columns=generic_men_category_cols)\n",
    "discriminative_metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cluster_df, discriminative_metrics = clusters_analysis(unusual_df, n_clusters=3, numerical_columns=unusual_numeric_cols, category_columns=unusual_category_cols)\n",
    "discriminative_metrics"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
