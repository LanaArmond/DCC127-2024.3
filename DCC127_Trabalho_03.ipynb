{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importando Bibliotecas\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from AnalysisUtils import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.impute import KNNImputer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def encode_string_columns(df):\n",
    "  # Select columns of type 'object' or 'category'\n",
    "  non_numeric_columns = df.select_dtypes(include=['object', 'category'])\n",
    "\n",
    "  for col in non_numeric_columns.columns:\n",
    "    # Get the unique values in the column\n",
    "    unique_values = df[col].dropna().unique()\n",
    "    \n",
    "    if col == 'come_entre_refeicoes':\n",
    "      df['dummy_come_entre_refeicoes'] = df[col].str.lower().map({'nao':0, 'as vezes':1, 'frequentemente':2, 'sempre': 3})\n",
    "      continue\n",
    "      \n",
    "    if col == 'consumo_alcool':\n",
    "      df['dummy_come_entre_refeicoes'] = df[col].str.lower().map({'nao':0, 'as vezes':1, 'frequentemente':2})\n",
    "      continue\n",
    "\n",
    "    if df[col].str.lower().isin(['sim', 'nao']).all():\n",
    "      df[f\"dummy_{col}\"] = df[col].str.lower().map({'sim': 1, 'nao': 0})\n",
    "      continue\n",
    "      \n",
    "    if len(unique_values) == 2:\n",
    "      label_encoder = LabelEncoder()\n",
    "      df[f\"dummy_{col}\"] = label_encoder.fit_transform(df[col])\n",
    "      continue\n",
    "    else:\n",
    "      df_dummies = pd.get_dummies(df[col], prefix=f\"dummy_{col}\")\n",
    "      df = pd.concat([df, df_dummies], axis=1)\n",
    "      continue\n",
    "        \n",
    "  return df\n",
    "\n",
    "def fill_missing_data(df, method='simple', n_neighbors=5):\n",
    "  if method == 'simple':\n",
    "    for col in df.columns:\n",
    "      if df[col].dtype in ['float64', 'int64']:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "      else:\n",
    "        if df[col].notna().any():\n",
    "          df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "  elif method == 'knn':\n",
    "    # Separate numeric and non-numeric columns\n",
    "    df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
    "    df_non_numeric = df.select_dtypes(exclude=['float64', 'int64'])\n",
    "\n",
    "    # Apply get_dummies for non-numeric columns to perform one-hot encoding\n",
    "    df_non_numeric_dummies = pd.get_dummies(df_non_numeric, drop_first=False)\n",
    "\n",
    "    # Impute using KNN\n",
    "    imputer_numeric = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputer_non_numeric = KNNImputer(n_neighbors=1)\n",
    "    \n",
    "    df_imputed_numeric = pd.DataFrame(\n",
    "      imputer_numeric.fit_transform(df_numeric),\n",
    "      columns=df_numeric.columns,\n",
    "      index=df_numeric.index\n",
    "    )\n",
    "\n",
    "    df_imputed_non_numeric = pd.DataFrame(\n",
    "      imputer_non_numeric.fit_transform(df_non_numeric_dummies),\n",
    "      columns=df_non_numeric_dummies.columns,\n",
    "      index=df_non_numeric_dummies.index\n",
    "    )\n",
    "    # Reverse the one-hot encoding by getting the most frequent category for each column\n",
    "    df_non_numeric_imputed = pd.DataFrame(index=df_imputed_non_numeric.index)\n",
    "    for col in df_non_numeric.columns:\n",
    "      vals_col = df_non_numeric[col].dropna().unique()\n",
    "      \n",
    "      dummies = [col + '_' + str(val) for val in vals_col]\n",
    "      dummies_values = df_imputed_non_numeric[dummies].idxmax(axis=1).apply(lambda x: x.split('_')[-1])\n",
    "      df_non_numeric_imputed[col] = dummies_values\n",
    "\n",
    "    # Combine numeric and non-numeric back to the original DataFrame\n",
    "    df = pd.concat([df_imputed_numeric, df_non_numeric_imputed], axis=1)\n",
    "\n",
    "  else:\n",
    "    raise ValueError(\"Invalid method. Choose from 'simple' or 'knn'.\")\n",
    "\n",
    "  return df\n",
    "\n",
    "def detect_outliers(df):\n",
    "  z_scores = np.abs(zscore(df.select_dtypes(include=[np.number])))\n",
    "  outliers = z_scores > 3  # Consider values with Z-score greater than 3 as outliers\n",
    "  return outliers.sum()\n",
    "\n",
    "def analyze_numerical_column(row):\n",
    "  print_simple_metrics(row)\n",
    "  boxplot_with_quartiles(row, yscale='linear')\n",
    "  create_filtered_histograms(row, log=False, filters=None, color='blue', bins=100)\n",
    "\n",
    "def calculate_group_metric(df, category_columns, numerical_columns, metric='median'):\n",
    "  result_list = []\n",
    "\n",
    "  for i in range(len(category_columns)):\n",
    "    for value in df[category_columns[i]].unique():\n",
    "      group_df = df[df[category_columns[i]] == value]\n",
    "\n",
    "      if metric == 'median':\n",
    "        group_metric = group_df[numerical_columns].median()\n",
    "      else:\n",
    "        group_metric = group_df[numerical_columns].mean()\n",
    "\n",
    "      group_metric['category'] = f\"{category_columns[i]}: {value}\"\n",
    "      result_list.append(group_metric)\n",
    "\n",
    "  result_df = pd.DataFrame(result_list)\n",
    "\n",
    "  return result_df\n",
    "\n",
    "def plot_sorted_group_metric(result_df, numerical_columns, metric='median'):\n",
    "  for col in numerical_columns:\n",
    "    # Sort the result DataFrame by the median of the current column\n",
    "    sorted_df = result_df.set_index('category').sort_values(by=col).reset_index()\n",
    "\n",
    "    # Create a new figure for each numerical column\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Prepare the boxplot data by creating a new DataFrame with the sorted categories\n",
    "    sorted_categories = sorted_df['category'].values\n",
    "    boxplot_data = []\n",
    "\n",
    "    for category in sorted_categories:\n",
    "      # Create the boxplot data by selecting the group corresponding to each category\n",
    "      group_category = category.split(\":\")[0]  # Extract the category name from 'column: value'\n",
    "      group_value = category.split(\":\")[1].strip()  # Extract the value\n",
    "      group_df = df[df[group_category] == group_value]  # Get the subgroup\n",
    "      boxplot_data.append(group_df[col].values)\n",
    "\n",
    "    # Create the boxplot, aligned by the sorted categories\n",
    "    sns.boxplot(data=boxplot_data, order=range(len(sorted_categories)))\n",
    "\n",
    "    # Set labels and title for the plot\n",
    "    plt.xticks(range(len(sorted_categories)), sorted_categories, rotation=45, ha='right')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel(f'{metric} of {col}')\n",
    "    plt.title(f'Sorted {metric}s of {col} by Category')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_category_columns(df, category_columns, color='skyblue', top=10):\n",
    "  num_columns = len(category_columns)\n",
    "  rows = (num_columns // 3) + (num_columns % 3 > 0)\n",
    "\n",
    "  fig, axes = plt.subplots(rows, 3, figsize=(15, 5 * rows))\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for idx, column in enumerate(category_columns):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Get the value counts for the encoded column\n",
    "    result_df = df[column].value_counts().reset_index()\n",
    "    result_df.columns = [column, 'Count']\n",
    "    result_df['Percentage'] = (result_df['Count'] / result_df['Count'].sum()) * 100\n",
    "\n",
    "    # Keep only top N values if specified\n",
    "    result_df = result_df.head(top)\n",
    "\n",
    "    # Plot the bar chart\n",
    "    result_df.plot(kind='bar', x=column, y='Count', color=color, ax=ax, legend=False)\n",
    "\n",
    "    ax.set_title(f\"Value Counts of {column}\", fontsize=12, pad=20)\n",
    "    ax.set_xlabel('Values', fontsize=10)\n",
    "    ax.set_ylabel('Count', fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for i, (count, pct) in enumerate(zip(result_df['Count'], result_df['Percentage'])):\n",
    "      ax.text(i, count + 0.5, f'{count} / {pct:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "  # Remove any empty axes\n",
    "  for i in range(idx + 1, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "  plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "def scale_data(df, columns):\n",
    "  scaler = StandardScaler()\n",
    "  df[columns] = scaler.fit_transform(df[columns])\n",
    "  return df\n",
    "\n",
    "def scale_and_prepare_data(df, categorical_columns, numerical_columns):\n",
    "  # One-hot encode categorical columns\n",
    "  df_dummies = pd.get_dummies(df[categorical_columns], drop_first=True)\n",
    "\n",
    "  # Initialize StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "\n",
    "  # Scale the specified numerical columns\n",
    "  df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "  # Scale the one-hot encoded categorical columns (df_dummies)\n",
    "  df_dummies_scaled = scaler.fit_transform(df_dummies)\n",
    "\n",
    "  # Convert the scaled dummies back to a DataFrame and assign correct column names manually\n",
    "  df_dummies_scaled = pd.DataFrame(df_dummies_scaled, columns=df_dummies.columns, index=df.index)\n",
    "\n",
    "  # Concatenate the scaled numerical columns with the scaled one-hot encoded columns\n",
    "  df = pd.concat([df[numerical_columns], df_dummies_scaled], axis=1)\n",
    "\n",
    "  return df\n",
    "\n",
    "def add_health_columns(df):\n",
    "    # Calcular IMC\n",
    "    df['IMC'] = df['peso'] / (df['altura'] ** 2)\n",
    "\n",
    "    # Classificar IMG\n",
    "    def classify_imc(img):\n",
    "        if img <= 18.5:\n",
    "            return 'Baixo'\n",
    "        elif img <= 24.9:\n",
    "            return 'Normal'\n",
    "        elif img <= 29.9:\n",
    "            return 'Sobrepeso'\n",
    "        else:\n",
    "            return 'Obesidade'\n",
    "\n",
    "    df['Class_IMC'] = df['IMC'].apply(classify_imc)\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_and_return_correlation(df, category_columns_dummies, numeric_columns, method='spearman', plot=True):\n",
    "\n",
    "  df_numeric = df[numeric_columns + category_columns_dummies]\n",
    "\n",
    "  correlation_matrix = df_numeric.corr(method=method)\n",
    "\n",
    "  # Plot the heatmap\n",
    "  if plot:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(f'Matriz de Correlação ({method.capitalize()})')\n",
    "    plt.show()\n",
    "\n",
    "  # Flatten the correlation matrix and sort by absolute value\n",
    "  corr_pairs = correlation_matrix.unstack()\n",
    "  sorted_corr = corr_pairs.sort_values(key=lambda x: x.abs(), ascending=False)\n",
    "\n",
    "  # Exclude self-correlations\n",
    "  filtered_corr = sorted_corr[sorted_corr.index.get_level_values(0) != sorted_corr.index.get_level_values(1)]\n",
    "\n",
    "  # Remove duplicates like [A, B] and [B, A]\n",
    "  unique_pairs = filtered_corr.reset_index()\n",
    "  unique_pairs['sorted_index'] = unique_pairs.apply(lambda row: tuple(sorted([row['level_0'], row['level_1']])), axis=1)\n",
    "  unique_pairs = unique_pairs.drop_duplicates(subset='sorted_index').drop(columns='sorted_index')\n",
    "\n",
    "  # Rename columns for clarity\n",
    "  unique_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "  return unique_pairs, correlation_matrix\n",
    "\n",
    "def show_changed_missing_values(df1, df2):\n",
    "\n",
    "  df1_ordered = df1[df2.columns]\n",
    "  changed = df1_ordered != df2\n",
    "  changed_rows_df1 = df1_ordered[changed].dropna(how='all')\n",
    "  changed_rows_df2 = df2[changed].dropna(how='all')\n",
    "\n",
    "  merged_values = changed_rows_df1.combine(\n",
    "    changed_rows_df2,\n",
    "    lambda x1, x2: pd.Series([f\"{v1} | {v2}\" if pd.notna(v1) and pd.notna(v2) else None for v1, v2 in zip(x1, x2)])\n",
    "  )\n",
    "\n",
    "  return merged_values\n",
    "\n",
    "def pca_analysis(df, columns, plot=True):\n",
    "  # Performing PCA\n",
    "  pca_df = df[columns].copy()\n",
    "  pca = PCA()\n",
    "  pca_data = pca.fit_transform(pca_df)\n",
    "  for i in range(pca_data.shape[1]):\n",
    "    pca_df[f'pca{i+1}'] = pca_data[:, i]\n",
    "\n",
    "  # Loading components\n",
    "  loadings = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=pca_df.columns[:pca.components_.shape[-1]],\n",
    "    index=[f'PCA{i + 1}' for i in range(pca.n_components_)]\n",
    "  )\n",
    "\n",
    "  # Explained variance ratio and PCA contributions\n",
    "  explained_variance_ratio = pca.explained_variance_ratio_\n",
    "  pca_contributions = (explained_variance_ratio[:5]).round(4)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pca_df['pca1'], pca_df['pca2'], s=50)\n",
    "    plt.title(\"PCA Clustering\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "  sorted_loadings = {}\n",
    "  for i in range(pca.n_components_):\n",
    "    pca_component = f'PCA{i + 1}'\n",
    "    top_contributors = loadings.iloc[i].abs().sort_values(ascending=False).head(5)\n",
    "\n",
    "    # Create a dictionary for the top contributors and their contributions\n",
    "    sorted_loadings[pca_component] = {\n",
    "      feature: top_contributors[feature] for feature in top_contributors.index\n",
    "    }\n",
    "\n",
    "  return pca_df, sorted_loadings, pca_contributions\n",
    "\n",
    "def pca_explained_variance_plot(df, columns):\n",
    "  pca_df = df[columns].copy()\n",
    "  pca = PCA()\n",
    "  pca_data = pca.fit_transform(pca_df)\n",
    "  explained_variance_ratio = pca.explained_variance_ratio_\n",
    "  cumulative_explained_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "  # Plot the cumulative explained variance vs the number of principal components\n",
    "  plt.figure(figsize=(8, 6))\n",
    "  plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='-')\n",
    "  plt.title(\"Cumulative Explained Variance vs. Number of Principal Components\")\n",
    "  plt.xlabel(\"Number of Principal Components\")\n",
    "  plt.ylabel(\"Cumulative Explained Variance (%)\")\n",
    "  plt.xticks(range(1, len(explained_variance_ratio) + 1))\n",
    "  plt.yticks([i / 10 for i in range(0, 11)])\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  # Now, let's calculate and plot the contribution of each original column to the total variance explained\n",
    "  loadings = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=pca_df.columns[:pca.components_.shape[-1]],\n",
    "    index=[f'PCA{i + 1}' for i in range(pca.n_components_)]\n",
    "  )\n",
    "  \n",
    "  # Calculate the contribution of each original feature across all principal components\n",
    "  feature_contributions = loadings.abs().sum(axis=0).sort_values(ascending=False)\n",
    "  \n",
    "  plt.figure(figsize=(12, 8))  # Set larger figure size\n",
    "  feature_contributions.plot(kind='bar', stacked=True, colormap='viridis', figsize=(12, 8))\n",
    "  plt.title(\"Feature Contribution to PCA Components\", fontsize=16, fontweight='bold')\n",
    "  plt.xlabel(\"Original Features\", fontsize=12)\n",
    "  plt.ylabel(\"Total Contribution to PCA Components\", fontsize=12)\n",
    "  plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "  plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  return cumulative_explained_variance, feature_contributions\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerical_columns = ['idade', 'idade_int', 'altura', 'peso', 'consome_vegetais', 'consome_vegetais_int', 'n_refeicoes_int', 'consumo_diario_agua', 'consumo_diario_agua_int', 'frequencia_atividade_fisica', 'frequencia_atividade_fisica_int', 'tempo_usando_eletronicos', 'tempo_usando_eletronicos_int', 'IMC']\n",
    "\n",
    "category_columns = ['sexo', 'historico_obesidade_familia', 'consome_comida_calorica', 'come_entre_refeicoes', 'fuma', 'consumo_alcool', 'tipo_transporte', 'Class_IMC', 'tipo_transporte_adaptado']\n",
    "\n",
    "category_columns_dummies = [\n",
    "  'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_fuma',\n",
    "  'dummy_tipo_transporte_adaptado_ativo',\n",
    "  'dummy_tipo_transporte_adaptado_individual',\n",
    "  'dummy_tipo_transporte_adaptado_transporte publico'\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparação de estratégias de inputting"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_test = pd.read_csv('data/trabalho2_dados_4.csv')\n",
    "missing_rows_idx = df_test[df_test.isna().any(axis=1)].index.tolist()\n",
    "\n",
    "df_test_knn = fill_missing_data(df_test, method='knn')\n",
    "df_test_simple = fill_missing_data(df_test, method='simple')\n",
    "\n",
    "show_changed_missing_values(df_test_knn, df_test_simple)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## buscando duplicada nova gerada com inputting (gerou para ambos)\n",
    "ele tinha transporte publico como NaN, mas ao substiruí-lo virou uma duplicada perfeita."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# analisando duplicata nova gerada pelo fill_missing_data\n",
    "df_test = pd.read_csv('data/trabalho2_dados_4.csv')\n",
    "\n",
    "print('Antes do Input')\n",
    "new_duplicate = df_test.loc[602].copy()\n",
    "display(df_test[df_test.eq(df_test.loc[1]).all(axis=1)])\n",
    "\n",
    "df_test = fill_missing_data(df_test) \n",
    "\n",
    "print('Depois do Input')\n",
    "display(df_test.loc[602])\n",
    "display(df_test[df_test.eq(df_test.loc[1]).all(axis=1)])\n",
    "new_duplicate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre processando Dados"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importando dados\n",
    "df = pd.read_csv('data/trabalho2_dados_4.csv')\n",
    "df = fill_missing_data(df, method='knn') # substituindo valores faltantes numéricos pela media e categóricos pela moda\n",
    "\n",
    "\n",
    "df['idade_int'] = np.floor(df['idade'])\n",
    "df['n_refeicoes_int'] = df['n_refeicoes'].round()\n",
    "df['consome_vegetais_int'] = df['consome_vegetais'].round()\n",
    "df['consumo_diario_agua_int'] = df['consumo_diario_agua'].round()\n",
    "df['frequencia_atividade_fisica_int'] = df['frequencia_atividade_fisica'].round()\n",
    "df['tempo_usando_eletronicos_int'] = df['tempo_usando_eletronicos'].round()\n",
    "\n",
    "df['tipo_transporte_adaptado'] = df['tipo_transporte'].apply(\n",
    "  lambda x: 'ativo' if x in ['bicicleta', 'andando'] else 'individual' if x in ['carro', 'moto'] else x)\n",
    "\n",
    "df = add_health_columns(df)\n",
    "df = encode_string_columns(df) # encodando colunas de string\n",
    "df = df.drop_duplicates(df, keep='first') # remoção de ~250 entradas duplicadas (substituição de valores faltantes e transformação direta em int nao criaram novas duplicados apenas 1 para o caso de substituição\n",
    "df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Detecção de outliers"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "detect_outliers(df) \n",
    "# Não foram detectados outliers relevantes nas métricas numéricas\n",
    "# a unica metrica com outliers usando z_score foi a de idade, mas essa distribuição não é normal, e foram apenas 7 encontrados\n",
    "# nas métricas categóricas, isso removeria certas do categorias do dataset, portanto, não é necessário remover outliers neste dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Análises simples"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_all_category_columns(df, category_columns, color='skyblue', top=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for row in numerical_columns:\n",
    "  analyze_numerical_column(df[row]) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gráficos categorias "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "result_df = calculate_group_metric(df, category_columns, numerical_columns, metric='mean')\n",
    "plot_sorted_group_metric(result_df, numerical_columns, metric='mean')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Análise de correlação com spearmann"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sorted_corr, corr_matrix = plot_and_return_correlation(df, category_columns_dummies=category_columns_dummies, numeric_columns=['idade_int', 'altura', 'peso', 'consome_vegetais_int', 'n_refeicoes_int', 'consumo_diario_agua_int', 'frequencia_atividade_fisica_int', 'tempo_usando_eletronicos_int', 'IMC'], method='spearman')\n",
    "sorted_corr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# analise de influencia de colunas int para correlação"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sorted_corr, corr_matrix = plot_and_return_correlation(df, category_columns_dummies=category_columns_dummies,\n",
    "                            numeric_columns=['idade_int', 'consome_vegetais_int', 'n_refeicoes_int',\n",
    "                                             'consumo_diario_agua_int', 'frequencia_atividade_fisica_int',\n",
    "                                             'tempo_usando_eletronicos_int'], method='spearman', plot=False)\n",
    "\n",
    "sorted_corr2, corr_matrix2 = plot_and_return_correlation(df, category_columns_dummies=category_columns_dummies,\n",
    "                                                       numeric_columns=['idade', 'consome_vegetais', 'n_refeicoes', 'consumo_diario_agua','frequencia_atividade_fisica', 'tempo_usando_eletronicos'],\n",
    "                                                       method='spearman', plot=False)\n",
    "\n",
    "\n",
    "diff = abs(corr_matrix.to_numpy()) - abs(corr_matrix2.to_numpy())\n",
    "mask1 = (abs(corr_matrix.to_numpy()) >= 0.3) & (diff > 0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(mask, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(f'int vs sem int  (diff>0)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mask2 = (abs(corr_matrix.to_numpy()) >= 0.3) & (diff < 0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(mask2, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(f'int vs sem int (diff<0)')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# começando analises para agrupamento"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerical_columns_clustering_refinado = ['idade_int', 'consome_vegetais_int', 'n_refeicoes_int', 'consumo_diario_agua_int', 'frequencia_atividade_fisica_int', 'IMC'] \n",
    "numerical_columns_clustering = ['idade', 'consome_vegetais', 'n_refeicoes', 'consumo_diario_agua', 'frequencia_atividade_fisica', 'tempo_usando_eletronicos', 'IMC', 'peso', 'altura']\n",
    "\n",
    "category_columns_clustering = [\n",
    "  'dummy_sexo',\n",
    "  'dummy_historico_obesidade_familia',\n",
    "  'dummy_consome_comida_calorica',\n",
    "  'dummy_come_entre_refeicoes',\n",
    "  'dummy_fuma',\n",
    "  'dummy_tipo_transporte_adaptado_ativo',\n",
    "  'dummy_tipo_transporte_adaptado_individual',\n",
    "  'dummy_tipo_transporte_adaptado_transporte publico'\n",
    "]\n",
    "\n",
    "print(f\"dimensionalidade: {len(numerical_columns_clustering + category_columns_clustering)}\")\n",
    "\n",
    "scaled_df = scale_data(df, list(set(numerical_columns_clustering + category_columns_clustering + numerical_columns_clustering_refinado)))\n",
    "scaled_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pca_analysis_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_clustering_refinado + category_columns_clustering)\n",
    "\n",
    "print(f\"contribuiçoes de cada pca: {pca_contributions}\")\n",
    "loadings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pca_analysis_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_clustering + category_columns_clustering)\n",
    "\n",
    "print(f\"contribuiçoes de cada pca: {pca_contributions}\")\n",
    "loadings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Baseado no gráfico, podemos remover o último pca sem problemas.\")\n",
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(df, columns=numerical_columns_clustering_refinado + category_columns_clustering)\n",
    "\n",
    "\n",
    "print(\"Baseado no gráfico, podemos remover o último pca sem problemas.\")\n",
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(df, columns=numerical_columns_clustering + category_columns_clustering)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pca_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns_clustering_refinado + category_columns_clustering, plot=False)\n",
    "\n",
    "pca_df = pca_df[[f\"pca{i}\"for i in range(1, 14)]]\n",
    "pca_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Clustering\n",
    "algoritmos selecionados:\n",
    "particionamento (kmeans)\n",
    "hierárquica (AGNES),\n",
    "densidade (optics)\n",
    "grade (clique)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
