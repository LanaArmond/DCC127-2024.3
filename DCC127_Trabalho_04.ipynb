{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_p_value(rule):\n",
    "    N = len(df)\n",
    "    support_a_b = rule['support']\n",
    "    support_a = rule['antecedent support']\n",
    "    support_b = rule['consequent support']\n",
    "\n",
    "    # Criando tabela de contingÃªncia\n",
    "    a_and_b = support_a_b * N\n",
    "    a_not_b = (support_a - support_a_b) * N\n",
    "    not_a_b = (support_b - support_a_b) * N\n",
    "    not_a_not_b = (1 - (support_a + support_b - support_a_b)) * N\n",
    "\n",
    "    contingency_table = [[a_and_b, a_not_b], [not_a_b, not_a_not_b]]\n",
    "    print(contingency_table)\n",
    "    print('------')\n",
    "\n",
    "    # Aplicando o teste de Fisher\n",
    "    _, p_value = stats.fisher_exact(contingency_table)\n",
    "    return p_value\n",
    "\n",
    "def eclat(df, min_support):\n",
    "    vertical_db = {item: set(df.index[df[item] == 1]) for item in df.columns}\n",
    "\n",
    "    def recursive_eclat(prefix, items):\n",
    "        results = []\n",
    "        for i, (item, tids) in enumerate(items):\n",
    "            new_prefix = prefix + [item]\n",
    "            new_tids = tids\n",
    "            support = len(new_tids)\n",
    "            relative_support = support / len(df)\n",
    "\n",
    "            if relative_support >= min_support:\n",
    "                results.append((set(new_prefix), support, relative_support))\n",
    "                new_items = [(other, other_tids & new_tids) for other, other_tids in items[i+1:] if len(other_tids & new_tids) >= min_support]\n",
    "                results.extend(recursive_eclat(new_prefix, new_items))\n",
    "\n",
    "        return results\n",
    "\n",
    "    frequent_itemsets = recursive_eclat([], sorted(vertical_db.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "\n",
    "    return pd.DataFrame(frequent_itemsets, columns=[\"itemsets\", \"support (freq)\", \"support\"])\n",
    "\n",
    "def plot_association_rules_graph(df, binary_df, figsize=(12, 8)):\n",
    "    df = df.sort_values(by=\"confidence\", ascending=False)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    support_values = df[\"support\"].values\n",
    "    confidence_values = df[\"confidence\"].values\n",
    "    edge_widths = 3\n",
    "\n",
    "    all_labels = {}\n",
    "    all_dashed_edges = []\n",
    "    edge_labels = {}  # Dictionary to hold support labels for solid edges\n",
    "\n",
    "    # Get the min and max lift values for normalization\n",
    "    min_lift = df[\"lift\"].min()\n",
    "    max_lift = df[\"lift\"].max()\n",
    "\n",
    "    # Normalize lift values using Matplotlib's Normalize\n",
    "    lift_norm = mcolors.Normalize(vmin=min_lift, vmax=max_lift)\n",
    "\n",
    "    # Get the min and max support values for normalization (for node size)\n",
    "    min_support = df[\"support\"].min()\n",
    "    max_support = df[\"support\"].max()\n",
    "    support_norm = mcolors.Normalize(vmin=min_support, vmax=max_support)\n",
    "\n",
    "    # Color map for solid edges (based on lift)\n",
    "    solid_edge_cmap = plt.cm.RdYlGn\n",
    "\n",
    "    # Color map for dashed edges (based on antecedent support)\n",
    "    dashed_edge_cmap = plt.cm.Blues  # Different color map for dashed edges\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        antecedent = row[\"antecedents\"]\n",
    "        consequent = row[\"consequents\"]\n",
    "        lift = row[\"lift\"]  # Directly use the lift value from the DataFrame\n",
    "\n",
    "        # Split sets into individual nodes and add edges\n",
    "        antecedent_nodes = list(antecedent)\n",
    "        consequent_nodes = list(consequent)\n",
    "\n",
    "        # Add the nodes to the graph with support calculated from the binary_df\n",
    "        for node in antecedent_nodes + consequent_nodes:\n",
    "            if node not in G.nodes:\n",
    "                # Calculate the support from the binary_df\n",
    "                node_support = binary_df[node].mean()  # Support is the mean of the binary values (frequency of 1s)\n",
    "                G.add_node(node, support=node_support, antecedent=row[\"antecedent support\"], consequent=row[\"consequent support\"])\n",
    "\n",
    "            # Add node label with its support value\n",
    "            if node in G.nodes:\n",
    "                node_support = G.nodes[node][\"support\"]  # Directly access the support value\n",
    "                all_labels[node] = f\"{node}\\nS: {node_support*100:.1f}%\"\n",
    "\n",
    "        # Determine the color based on the normalized lift value (for solid edges)\n",
    "        edge_color = solid_edge_cmap(lift_norm(lift))  # Normalize lift to color scale\n",
    "\n",
    "        # If antecedent or consequent has multiple items, handle dashed edges\n",
    "        if len(antecedent_nodes) > 1 or len(consequent_nodes) > 1:\n",
    "            # Define dashed edges for multiple items, with support based on antecedent support\n",
    "            for a in antecedent_nodes:\n",
    "                for c in consequent_nodes:\n",
    "                    all_dashed_edges.append((a, c))  # Add edge to dashed list\n",
    "        else:\n",
    "            # Otherwise, add a solid edge with normal support\n",
    "            for a in antecedent_nodes:\n",
    "                for c in consequent_nodes:\n",
    "                    G.add_edge(a, c, weight=row[\"antecedent support\"], confidence=row[\"confidence\"], lift=lift, color=edge_color)\n",
    "                    # Store support labels for solid edges\n",
    "                    edge_labels[(a, c)] = f\"C: {row['confidence']*100:.1f}%\"  # Add support as label for solid edges\n",
    "\n",
    "    # Get weakly connected components (instead of strongly connected)\n",
    "    connected_components = list(nx.weakly_connected_components(G))\n",
    "\n",
    "    # Plot each connected component separately\n",
    "    for i, component in enumerate(connected_components):\n",
    "        if len(component) > 1:  # Only plot components with more than 1 node\n",
    "            # Create a subgraph for the component\n",
    "            subgraph = G.subgraph(component)\n",
    "\n",
    "            # Node positions using spring layout for the subgraph (not the entire graph)\n",
    "            pos = nx.spring_layout(subgraph, seed=42)\n",
    "\n",
    "            # Initialize labels for the subgraph (filtering to include only nodes in the component)\n",
    "            subgraph_labels = {node: all_labels[node] for node in component}\n",
    "\n",
    "            # Filter dashed edges to only include edges in the subgraph\n",
    "            subgraph_dashed_edges = [(u, v) for u, v in all_dashed_edges if u in component and v in component]\n",
    "\n",
    "            plt.figure(figsize=figsize)\n",
    "\n",
    "            # Compute node sizes based only on the subgraph's nodes (based on support)\n",
    "            node_sizes = [\n",
    "                100 + 10 * (support_norm(G.nodes[node][\"support\"]) - min_support) / (max_support - min_support)\n",
    "                for node in subgraph.nodes()\n",
    "            ]\n",
    "\n",
    "            # Draw nodes with sizes based on support\n",
    "            nx.draw_networkx_nodes(subgraph, pos, node_color=\"lightgray\", node_size=node_sizes)\n",
    "\n",
    "            # Draw solid edges with color based on lift (no change in edge width)\n",
    "            solid_edges = [e for e in subgraph.edges() if (e[0], e[1]) not in subgraph_dashed_edges]\n",
    "            edge_colors = [subgraph[u][v][\"color\"] for u, v in solid_edges]\n",
    "            nx.draw_networkx_edges(subgraph, pos, edgelist=solid_edges, edge_color=edge_colors, width=3, arrows=True, arrowsize=20)\n",
    "\n",
    "            # Draw dashed edges for multi-item sets and include support label for each dashed edge\n",
    "            dashed_edge_labels = {}\n",
    "            dashed_edge_colors = []\n",
    "            for u, v in subgraph_dashed_edges:\n",
    "                dashed_support = len(binary_df[(binary_df[u]) & (binary_df[v])])/len(binary_df)\n",
    "                dashed_edge_labels[(u, v)] = f\"S: {dashed_support*100:.1f}%\"  # Add support as label\n",
    "                # Assign a different color map for dashed edges (based on antecedent support)\n",
    "                dashed_edge_colors.append(dashed_edge_cmap(support_norm(dashed_support)))  # Different color map for dashed edges\n",
    "            nx.draw_networkx_edges(subgraph, pos, edgelist=[(u, v) for u, v in subgraph_dashed_edges], edge_color=dashed_edge_colors, width=3, style=\"dashed\", arrows=True, arrowsize=20)\n",
    "\n",
    "            # Draw edge labels for dashed edges with support value\n",
    "            nx.draw_networkx_edge_labels(subgraph, pos, edge_labels=dashed_edge_labels, font_size=8, font_color=\"black\")\n",
    "\n",
    "            # Draw edge labels for solid edges with support value\n",
    "            # Ensure that only edges present in the subgraph are labeled\n",
    "            edge_labels_filtered = {k: v for k, v in edge_labels.items() if k[0] in component and k[1] in component}\n",
    "            nx.draw_networkx_edge_labels(subgraph, pos, edge_labels=edge_labels_filtered, font_size=8, font_color=\"black\")\n",
    "\n",
    "            # Draw node labels with support values\n",
    "            nx.draw_networkx_labels(subgraph, pos, labels=subgraph_labels, font_size=8, font_color=\"black\")\n",
    "\n",
    "            # Colorbar for true lift values (not normalized)\n",
    "            sm = cm.ScalarMappable(cmap=\"RdYlGn\", norm=lift_norm)  # Use normalized color scale\n",
    "            sm.set_array([])  # Empty array for the colorbar\n",
    "            cbar = plt.colorbar(sm, ax=plt.gca(), fraction=0.03, pad=0.02)\n",
    "            cbar.set_label(\"Lift\", fontsize=12)\n",
    "            cbar.set_ticks([min_lift, (min_lift + max_lift) / 2, max_lift])  # Set ticks to reflect the true lift range\n",
    "\n",
    "            plt.title(f\"Association Rules Network Graph (Component {i + 1})\", fontsize=14)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/trabalho4_dados_4.csv')\n",
    "df = df.astype(bool)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets = apriori(df, min_support=0.01, use_colnames=True)\n",
    "itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total de itemsets encontrados: {len(itemsets)}\")\n",
    "print(f\"Mediana de suporte para itemsets: {itemsets['support'].median()}\")\n",
    "print(f\"75% maiores suportes: {itemsets['support'].quantile(0.75)}\")\n",
    "print(f\"90% maiores suportes: {itemsets['support'].quantile(0.90)}\")\n",
    "print(f\"95% maiores suportes: {itemsets['support'].quantile(0.95)}\")\n",
    "itemsets['support'].hist(bins=100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets = apriori(df, min_support=0.02, use_colnames=True)\n",
    "regras = association_rules(itemsets, len(df), metric=\"confidence\", min_threshold=0.01)\n",
    "regras.sort_values(by=\"lift\", ascending=False)\n",
    "regras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total de regras encontradas: {len(regras)}\")\n",
    "print(f\"Mediana de confiaÃ§a para regras: {regras['confidence'].median()}\")\n",
    "print(f\"75% maiores confianÃ§as: {regras['confidence'].quantile(0.75)}\")\n",
    "print(f\"90% maiores confianÃ§as: {regras['confidence'].quantile(0.90)}\")\n",
    "print(f\"95% maiores confianÃ§as: {regras['confidence'].quantile(0.95)}\")\n",
    "regras['confidence'].hist(bins=100)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets = apriori(df, min_support=0.02, use_colnames=True)\n",
    "regras = association_rules(itemsets, len(df), metric=\"confidence\", min_threshold=0.5)\n",
    "regras.sort_values(by=\"lift\", ascending=False)\n",
    "regras['p-value'] = regras.apply(fisher_p_value, axis=1)\n",
    "regras  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets = eclat(df, min_support=0.02)\n",
    "regras = association_rules(itemsets, len(df), metric=\"confidence\", min_threshold=0.2)\n",
    "regras = regras[(regras['confidence']*regras['support'] >= 0.01) & (regras['lift'] >= 1.1)]\n",
    "regras.sort_values(by=\"confidence\", ascending=False)\n",
    "regras['p-value'] = regras.apply(fisher_p_value, axis=1)\n",
    "regras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regras.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_association_rules_graph(regras, df)  # Pass both the df with rules and the binary DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
