{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho 05 - Classificação"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from  scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from AnalysisUtils import print_simple_metrics, boxplot_with_quartiles, create_filtered_histograms\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_rel\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_data():\n",
    "    return pd.read_csv('data/trabalho5_dados_4.csv')\n",
    "\n",
    "def preprocess_data(df, categorical_columns, inputting_method):\n",
    "    display(((df == 'MD') | df.isna()).sum())\n",
    "    df = df.replace('MD', np.nan)\n",
    "    df = fill_missing_data(df, method=inputting_method)\n",
    "\n",
    "    df['Quantitative Ability 1'] = df['Quantitative Ability 1'].astype(float)\n",
    "    df['Analytical Skills 1'] = df['Analytical Skills 1'].astype(float)\n",
    "    df['Domain Skills 1'] = df['Domain Skills 1'].astype(float)\n",
    "\n",
    "    df['Year of Birth'] = df['Year of Birth'].map(lambda x: x[1:]).astype(int)\n",
    "    df['10th Completion Year'] = df['10th Completion Year'].map(\n",
    "        lambda x: x[1:]).astype(int)\n",
    "    df['12th Completion year'] = df['12th Completion year'].map(\n",
    "        lambda x: x[1:]).astype(int)\n",
    "    df[' Year of Completion of college'] = df[' Year of Completion of college'].map(\n",
    "        lambda x: x[1:]).astype(int)\n",
    "\n",
    "    df.rename(columns={\n",
    "              ' Year of Completion of college': 'Year of Completion of college'}, inplace=True)\n",
    "    df.rename(columns={' 10th percentage': '10th percentage'}, inplace=True)\n",
    "    df.rename(columns={' 12th percentage': '12th percentage'}, inplace=True)\n",
    "    df.rename(\n",
    "        columns={' College percentage': 'College percentage'}, inplace=True)\n",
    "    df.rename(columns={' English 1': 'English 1'}, inplace=True)\n",
    "\n",
    "    df = encode_string_columns(df, categorical_columns)\n",
    "\n",
    "    # As linhas estão realmentes duplicadas (Candidate ID iguais)\n",
    "    # duplicates = df[df.duplicated(keep='first') | df.duplicated(keep='last')]\n",
    "    # duplicates = duplicates.sort_values(by='Candidate ID')\n",
    "    # display(HTML(duplicates.to_html()))\n",
    "\n",
    "    df = df.drop_duplicates(df, keep='first')  # Removeu 55 duplicatas\n",
    "\n",
    "    return df\n",
    "\n",
    "def fill_missing_data(df, method='knn', n_neighbors=5):\n",
    "    if method == 'simple':\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype in ['float64', 'int64']:\n",
    "                df[col] = df[col].fillna(df[col].mean())\n",
    "            else:\n",
    "                if df[col].notna().any():\n",
    "                    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    elif method == 'knn':\n",
    "        # Separate numeric and non-numeric columns\n",
    "        df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
    "        df_non_numeric = df.select_dtypes(exclude=['float64', 'int64'])\n",
    "\n",
    "        # Apply get_dummies for non-numeric columns to perform one-hot encoding\n",
    "        df_non_numeric_dummies = pd.get_dummies(\n",
    "            df_non_numeric, drop_first=False)\n",
    "\n",
    "        # Impute using KNN\n",
    "        imputer_numeric = KNNImputer(n_neighbors=n_neighbors)\n",
    "        imputer_non_numeric = KNNImputer(n_neighbors=1)\n",
    "\n",
    "        df_imputed_numeric = pd.DataFrame(\n",
    "            imputer_numeric.fit_transform(df_numeric),\n",
    "            columns=df_numeric.columns,\n",
    "            index=df_numeric.index\n",
    "        )\n",
    "\n",
    "        df_imputed_non_numeric = pd.DataFrame(\n",
    "            imputer_non_numeric.fit_transform(df_non_numeric_dummies),\n",
    "            columns=df_non_numeric_dummies.columns,\n",
    "            index=df_non_numeric_dummies.index\n",
    "        )\n",
    "        # Reverse the one-hot encoding by getting the most frequent category for each column\n",
    "        df_non_numeric_imputed = pd.DataFrame(\n",
    "            index=df_imputed_non_numeric.index)\n",
    "        for col in df_non_numeric.columns:\n",
    "            vals_col = df_non_numeric[col].dropna().unique()\n",
    "\n",
    "            dummies = [col + '_' + str(val) for val in vals_col]\n",
    "            dummies_values = df_imputed_non_numeric[dummies].idxmax(\n",
    "                axis=1).apply(lambda x: x.split('_')[-1])\n",
    "            df_non_numeric_imputed[col] = dummies_values\n",
    "\n",
    "        # Combine numeric and non-numeric back to the original DataFrame\n",
    "        df = pd.concat([df_imputed_numeric, df_non_numeric_imputed], axis=1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from 'simple' or 'knn'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_string_columns(df, categorical_columns: list[str]):\n",
    "    for col in categorical_columns:\n",
    "        # Get the unique values in the column\n",
    "        unique_values = df[col].dropna().unique()\n",
    "\n",
    "        if col == 'Gender':\n",
    "            df['dummy_Gender'] = df[col].str.lower().map({'a': 0, 'b': 1})\n",
    "            continue\n",
    "\n",
    "        if col == 'Month of Birth':\n",
    "            df['dummy_Month of Birth'] = df[col].str.lower().map({\n",
    "                'jan': 0,\n",
    "                'feb': 1,\n",
    "                'mar': 2,\n",
    "                'apr': 3,\n",
    "                'may': 4,\n",
    "                'jun': 5,\n",
    "                'jul': 6,\n",
    "                'aug': 7,\n",
    "                'sep': 8,\n",
    "                'oct': 9,\n",
    "                'nov': 10,\n",
    "                'dec': 11,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if col == 'Performance':\n",
    "            df['Performance_classification'] = df[col].str.lower().map(\n",
    "                {'lp': 0, 'mp': 0, 'bp': 1})\n",
    "            continue\n",
    "\n",
    "        if len(unique_values) == 2:\n",
    "            label_encoder = LabelEncoder()\n",
    "            df[f\"dummy_{col}\"] = label_encoder.fit_transform(df[col])\n",
    "            continue\n",
    "        else:\n",
    "            df_dummies = pd.get_dummies(df[col], prefix=f\"dummy_{col}\")\n",
    "            df = pd.concat([df, df_dummies], axis=1)\n",
    "            continue\n",
    "\n",
    "    return df\n",
    "\n",
    "def detect_outliers_by_iiq(df, numerical_columns):\n",
    "    outlier_counts = {}\n",
    "\n",
    "    for col in numerical_columns:\n",
    "        col_values = df[col].dropna()\n",
    "        q1, q3 = np.percentile(col_values, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "        outlier_counts[col] = ((df[col] < lower_bound) |\n",
    "                               (df[col] > upper_bound)).sum()\n",
    "\n",
    "    return pd.Series(outlier_counts)\n",
    "\n",
    "def plot_all_category_columns(df, category_columns, class_column=None, top=10):\n",
    "    num_columns = len(category_columns)\n",
    "    rows = (num_columns // 3) + (num_columns % 3 > 0)\n",
    "    fig, axes = plt.subplots(rows, 3, figsize=(15, 5 * rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, column in enumerate(category_columns):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        if class_column:\n",
    "            # Group by both the category and class column\n",
    "            result_df = df.groupby([column, class_column]).size().reset_index(name='Count')\n",
    "            result_df = result_df.sort_values(by='Count', ascending=False)\n",
    "            top_values = result_df[column].value_counts().index[:top]\n",
    "            result_df = result_df[result_df[column].isin(top_values)]\n",
    "\n",
    "            # Use seaborn to create a color-coded bar plot\n",
    "            sns.barplot(data=result_df, x=column, y='Count', hue=class_column, ax=ax)\n",
    "            ax.legend(title=class_column)\n",
    "        else:\n",
    "            # Get the value counts for the column\n",
    "            result_df = df[column].value_counts().reset_index()\n",
    "            result_df.columns = [column, 'Count']\n",
    "            result_df['Percentage'] = (result_df['Count'] / result_df['Count'].sum()) * 100\n",
    "            result_df = result_df.head(top)\n",
    "\n",
    "            # Plot the bar chart\n",
    "            sns.barplot(data=result_df, x=column, y='Count', ax=ax, color='skyblue')\n",
    "\n",
    "            for i, (count, pct) in enumerate(zip(result_df['Count'], result_df['Percentage'])):\n",
    "                ax.text(i, count + 0.5, f'{count} / {pct:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "        ax.set_title(f\"Value Counts of {column}\", fontsize=12, pad=20)\n",
    "        ax.set_xlabel('Values', fontsize=10)\n",
    "        ax.set_ylabel('Count', fontsize=10)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Remove any empty axes\n",
    "    for i in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_smooth_histograms(df, numerical_column, class_column, bins=100, log=False):\n",
    "    print(f\"\\n--------------------------------------\\n\")\n",
    "    print(f\"Histograms for column {numerical_column} by {class_column} with bins: {bins}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot separate distributions for each class in class_column\n",
    "    unique_classes = df[class_column].dropna().unique()\n",
    "    colors = sns.color_palette(\"husl\", len(unique_classes))\n",
    "\n",
    "    for class_value, color in zip(unique_classes, colors):\n",
    "        class_data = df[df[class_column] == class_value][numerical_column]\n",
    "        sns.histplot(class_data, bins=bins, kde=True, color=color, log_scale=log, alpha=0.5, label=f\"{class_column}: {class_value}\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(numerical_column)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Distribution of {numerical_column} by {class_column}\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n--------------------------------------\\n\")\n",
    "\n",
    "def analyze_all_numerical_columns(df, numerical_columns, class_column):\n",
    "    for numerical_column in numerical_columns:\n",
    "        analyze_numerical_column(df, numerical_column, class_column)\n",
    "\n",
    "def analyze_numerical_column(df, numerical_column, class_column):\n",
    "    print_simple_metrics(df[numerical_column])\n",
    "    boxplot_with_quartiles(df[numerical_column], yscale='linear')\n",
    "    create_smooth_histograms(df, numerical_column=numerical_column, class_column=class_column, bins=100)\n",
    "\n",
    "def correlation_heatmap(df, numerical_columns):\n",
    "    plt.figure(figsize=(10, 8))  # Set figure size\n",
    "    sns.heatmap(df[numerical_columns].corr(), annot=True,\n",
    "                cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(\"Correlação dos Atributos\")\n",
    "    plt.show()\n",
    "\n",
    "def categorical_correlation_chi_square(df, categorical_columns):\n",
    "    corr_matrix = pd.DataFrame(np.ones((len(categorical_columns), len(categorical_columns))),\n",
    "                               index=categorical_columns, columns=categorical_columns)\n",
    "\n",
    "    for col1 in categorical_columns:\n",
    "        for col2 in categorical_columns:\n",
    "            if col1 != col2:\n",
    "                corr_matrix.loc[col1, col2] = chi_square_p_value(\n",
    "                    df[col1], df[col2])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm',\n",
    "                fmt=\".4f\", linewidths=0.5)\n",
    "    plt.title(\"Categorical Correlation Heatmap (Chi-Square p-values)\")\n",
    "    plt.show()\n",
    "\n",
    "def chi_square_p_value(x, y):\n",
    "    contingency_table = pd.crosstab(x, y)\n",
    "    _, p, _, _ = chi2_contingency(contingency_table)\n",
    "    return p\n",
    "\n",
    "def scale_data(df, columns):\n",
    "    scaler = StandardScaler()\n",
    "    new_df = df.copy()\n",
    "    new_df[columns] = scaler.fit_transform(df[columns])\n",
    "    return new_df\n",
    "\n",
    "def pca_analysis(df, columns, plot=True):\n",
    "    # Performing PCA\n",
    "    pca_df = df[columns].copy()\n",
    "    pca = PCA()\n",
    "    pca_data = pca.fit_transform(pca_df)\n",
    "    for i in range(pca_data.shape[1]):\n",
    "        pca_df[f'pca{i+1}'] = pca_data[:, i]\n",
    "\n",
    "    # Loading components\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_,\n",
    "        columns=pca_df.columns[:pca.components_.shape[-1]],\n",
    "        index=[f'PCA{i + 1}' for i in range(pca.n_components_)]\n",
    "    )\n",
    "\n",
    "    # Explained variance ratio and PCA contributions\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    pca_contributions = (explained_variance_ratio[:5]).round(4)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(pca_df['pca1'], pca_df['pca2'], s=50)\n",
    "        plt.title(\"PCA Clustering\")\n",
    "        plt.xlabel(\"PCA Component 1\")\n",
    "        plt.ylabel(\"PCA Component 2\")\n",
    "        plt.show()\n",
    "\n",
    "    sorted_loadings = {}\n",
    "    for i in range(pca.n_components_):\n",
    "        pca_component = f'PCA{i + 1}'\n",
    "        top_contributors = loadings.iloc[i].abs(\n",
    "        ).sort_values(ascending=False).head(5)\n",
    "\n",
    "        # Create a dictionary for the top contributors and their contributions\n",
    "        sorted_loadings[pca_component] = {\n",
    "            feature: top_contributors[feature] for feature in top_contributors.index\n",
    "        }\n",
    "\n",
    "    return pca_df, sorted_loadings, pca_contributions\n",
    "\n",
    "def pca_explained_variance_plot(df, columns):\n",
    "    pca_df = df[columns].copy()\n",
    "    pca = PCA()\n",
    "    pca_data = pca.fit_transform(pca_df)\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "    # Plot the cumulative explained variance vs the number of principal components\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(cumulative_explained_variance) + 1),\n",
    "             cumulative_explained_variance, marker='o', linestyle='-')\n",
    "    plt.title(\"Cumulative Explained Variance vs. Number of Principal Components\")\n",
    "    plt.xlabel(\"Number of Principal Components\")\n",
    "    plt.ylabel(\"Cumulative Explained Variance (%)\")\n",
    "    plt.xticks(range(1, len(explained_variance_ratio) + 1))\n",
    "    plt.yticks([i / 10 for i in range(0, 11)])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Now, let's calculate and plot the contribution of each original column to the total variance explained\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_,\n",
    "        columns=pca_df.columns[:pca.components_.shape[-1]],\n",
    "        index=[f'PCA{i + 1}' for i in range(pca.n_components_)]\n",
    "    )\n",
    "\n",
    "    # Calculate the contribution of each original feature across all principal components\n",
    "    feature_contributions = loadings.abs().sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))  # Set larger figure size\n",
    "    feature_contributions.plot(\n",
    "        kind='bar', stacked=True, colormap='viridis', figsize=(12, 8))\n",
    "    plt.title(\"Feature Contribution to PCA Components\",\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Original Features\", fontsize=12)\n",
    "    plt.ylabel(\"Total Contribution to PCA Components\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return cumulative_explained_variance, feature_contributions\n",
    "\n",
    "def train_classifier(df, model, columns, y_column, test_size=0.3):\n",
    "    X = df[columns]\n",
    "    y = df[y_column]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def stratified_k_fold_cross_validation(df, model, columns, y_column, k=10, scale=False):\n",
    "    steps = []\n",
    "    if scale:\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('classifier', model))\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # Define stratified k-fold\n",
    "    stratified_kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define Kappa as scoring metric\n",
    "    kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(pipeline, df[columns], df[y_column], cv=stratified_kf, scoring=kappa_scorer)\n",
    "\n",
    "    return scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numerical_columns = [\n",
    "    '10th percentage',\n",
    "    '12th percentage',\n",
    "    'College percentage',\n",
    "    'English 1',\n",
    "    'English 2',\n",
    "    'English 3',\n",
    "    'English 4',\n",
    "    'Quantitative Ability 1',\n",
    "    'Quantitative Ability 2',\n",
    "    'Quantitative Ability 3',\n",
    "    'Quantitative Ability 4',\n",
    "    'Domain Skills 1',\n",
    "    'Domain Skills 2',\n",
    "    'Domain Test 3',\n",
    "    'Domain Test 4',\n",
    "    'Analytical Skills 1',\n",
    "    'Analytical Skills 2',\n",
    "    'Analytical Skills 3', \n",
    "    'Year of Birth',\n",
    "    '10th Completion Year',\n",
    "    '12th Completion year',\n",
    "    'Year of Completion of college'\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    'Month of Birth',\n",
    "    'Gender',\n",
    "    'State (Location)',\n",
    "    'Degree of study',\n",
    "    'Specialization in study'\n",
    "]\n",
    "\n",
    "ignored_columns = [\n",
    "    'Candidate ID',\n",
    "    'Name',\n",
    "    'Number of characters in Original Name'\n",
    "]\n",
    "\n",
    "y_column = 'Performance_classification'\n",
    "y_columns = ['Performance', y_column]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "original_df = load_data()\n",
    "(original_df == 'MD').sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = preprocess_data(original_df, categorical_columns + ['Performance'], inputting_method='knn')\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[['Performance_classification', 'Performance']]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dummy_columns = [column for column in df.columns if column not in (numerical_columns + ignored_columns + y_columns + categorical_columns)]\n",
    "dummy_columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "detect_outliers_by_iiq(df, numerical_columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "plot_all_category_columns(df, categorical_columns, class_column='Performance_classification', top=40)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(df)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "analyze_all_numerical_columns(df, numerical_columns, class_column='Performance_classification')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "correlation_heatmap(df, numerical_columns)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "categorical_correlation_chi_square(df, numerical_columns + categorical_columns)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scaled_df = scale_data(df, numerical_columns + dummy_columns)\n",
    "scaled_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_df, loadings, pca_contributions = pca_analysis(scaled_df, columns=numerical_columns + dummy_columns)\n",
    "\n",
    "print(f\"contribuiçoes de cada pca: {pca_contributions}\")\n",
    "loadings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cumulative_explained_variance, feature_contributions = pca_explained_variance_plot(scaled_df, columns=numerical_columns + dummy_columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_df = pca_df[[f\"pca{i}\"for i in range(1, 40)]]\n",
    "pca_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "classification_columns = numerical_columns + dummy_columns\n",
    "classification_columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## stratified K fold cross validation with Train test split 70 - 30 "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_models(df, models, columns, y_column, pipelines=None, k=5):\n",
    "    results = []\n",
    "    stratified_kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    unique_classes = sorted(df[y_column].unique())\n",
    "    class_counts = df[y_column].value_counts(normalize=True) * 100\n",
    "\n",
    "    pbar = tqdm(models.items(), desc=\"Evaluating models\")\n",
    "    \n",
    "    # Loop through models\n",
    "    for model_name, model in pbar:\n",
    "        pbar.set_description(f\"Processing {model_name}\")\n",
    "        \n",
    "        for pipeline_name, pipeline_func in (pipelines or {}).items():\n",
    "            print(f\"initializing: {pipeline_name}\")\n",
    "            kappa_scores, accuracies = [], []\n",
    "            per_class_precision = {c: [] for c in unique_classes}\n",
    "            per_class_recall = {c: [] for c in unique_classes}\n",
    "\n",
    "            # Stratified K-fold loop\n",
    "            for train_idx, test_idx in stratified_kf.split(df, df[y_column]):\n",
    "                X_train, X_test = df.iloc[train_idx], df.iloc[test_idx]\n",
    "                y_train, y_test = df.iloc[train_idx][y_column], df.iloc[test_idx][y_column]\n",
    "\n",
    "                X_train, X_test, new_cols = pipeline_func(X_train, X_test, columns)\n",
    "\n",
    "                # Fit the model to the training data\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                # Store the results\n",
    "                kappa_scores.append(cohen_kappa_score(y_test, y_pred))\n",
    "                accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "                precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, labels=unique_classes, zero_division=0)\n",
    "\n",
    "                for i, c in enumerate(unique_classes):\n",
    "                    per_class_precision[c].append(precision[i])\n",
    "                    per_class_recall[c].append(recall[i])\n",
    "\n",
    "            # Add suffix for the applied pipeline\n",
    "            model_result = {\n",
    "                'Model': model_name,\n",
    "                'pipeline': pipeline_name,\n",
    "                'Mean Kappa Score': np.mean(kappa_scores),\n",
    "                'Std Kappa Score': np.std(kappa_scores),\n",
    "                'Mean Accuracy': np.mean(accuracies),\n",
    "                'Std Accuracy': np.std(accuracies)\n",
    "            }\n",
    "\n",
    "            # Add per-class precision and recall to the result\n",
    "            for c in unique_classes:\n",
    "                model_result[f'Precision Class {c}'] = np.mean(per_class_precision[c])\n",
    "                model_result[f'Recall Class {c}'] = np.mean(per_class_recall[c])\n",
    "                model_result[f'Class {c} Representation (%)'] = class_counts.get(c, 0)\n",
    "\n",
    "            results.append(model_result)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_random_forest_with_shap(df, columns, pipeline, y_column, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest model, evaluates it using SHAP, and plots feature importance as a vertical bar plot.\n",
    "    \"\"\"\n",
    "    # Extract the target variable\n",
    "    y = df[y_column]\n",
    "\n",
    "    # Apply the pipeline to transform the data\n",
    "    df_transformed, _, transformed_columns = pipeline(df, df, columns)\n",
    "    X = df_transformed[transformed_columns]\n",
    "    print(f\"Dimensionality: {len(transformed_columns)}\")\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Train a Random Forest model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = transformed_columns\n",
    "\n",
    "    # Sort feature importances in descending order\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Plot feature importance as a vertical bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Importance\")\n",
    "    plt.title(\"Feature Importance from Random Forest\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Create a SHAP explainer using X_train\n",
    "    explainer = shap.Explainer(model, X_train)\n",
    "\n",
    "    # Calculate SHAP values for X_train\n",
    "    shap_values = explainer(X_train, check_additivity=False)\n",
    "\n",
    "    # For binary classification, select SHAP values for the positive class (class 1)\n",
    "    if len(shap_values.shape) == 3:  # Multi-class\n",
    "        shap_values_selected = shap_values.values[:, :, 1]  # Select SHAP values for the positive class\n",
    "    else:  # Binary or regression\n",
    "        shap_values_selected = shap_values.values\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values_selected, X_train, feature_names=transformed_columns)\n",
    "    plt.title(\"SHAP Summary Plot for All Features (X_train)\")\n",
    "    plt.show()\n",
    "\n",
    "    return model, explainer, shap_values\n",
    "\n",
    "def plot_model_results(df):\n",
    "    \"\"\"\n",
    "    Given a dataframe with columns including:\n",
    "    'Model', 'pipeline', 'Mean Kappa Score', 'Std Kappa Score', 'Mean Accuracy', 'Std Accuracy',\n",
    "    'Precision Class 0', 'Recall Class 0', 'Precision Class 1', 'Recall Class 1',\n",
    "    this function:\n",
    "      1. Sorts models by Mean Kappa Score and selects the top 10 and bottom 10.\n",
    "      2. Creates bar plots for Mean Kappa Score and Mean Accuracy (both sorted separately).\n",
    "      3. Creates a sorted bar chart with two bars per model for Precision and Recall of Class 0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a unique identifier for each approach\n",
    "    df[\"Approach\"] = df[\"Model\"] + \" - \" + df[\"pipeline\"]\n",
    "\n",
    "    # ---- Sorting by Mean Kappa Score ----\n",
    "    df_sorted_kappa = df.sort_values(by=\"Mean Kappa Score\", ascending=False)\n",
    "    df_top10_kappa = df_sorted_kappa.head(10)\n",
    "    df_bottom10_kappa = df_sorted_kappa.tail(10)\n",
    "    df_kappa = pd.concat([df_top10_kappa, df_bottom10_kappa])\n",
    "\n",
    "    # ---- Sorting by Mean Accuracy ----\n",
    "    df_sorted_accuracy = df.sort_values(by=\"Mean Accuracy\", ascending=False)\n",
    "    df_top10_accuracy = df_sorted_accuracy.head(10)\n",
    "    df_bottom10_accuracy = df_sorted_accuracy.tail(10)\n",
    "    df_accuracy = pd.concat([df_top10_accuracy, df_bottom10_accuracy])\n",
    "\n",
    "    # ---- PLOT 1: Bar plot for Mean Kappa Score (Top & Bottom 10, sorted) ----\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_kappa, x='Approach', y='Mean Kappa Score', palette=\"viridis_r\")\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.title(\"Top & Bottom 10 Models: Mean Kappa Score (Sorted)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- PLOT 2: Bar plot for Mean Accuracy (Top & Bottom 10, sorted) ----\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_accuracy, x='Approach', y='Mean Accuracy', palette=\"viridis_r\")\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.title(\"Top & Bottom 10 Models: Mean Accuracy (Sorted)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- PLOT 3: Sorted Bar Chart for Precision & Recall (Class 0) ----\n",
    "    df_sorted_prec_recall = df_kappa.sort_values(by=['Precision Class 1'], ascending=False)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    df_melted_prec_recall = df_sorted_prec_recall.melt(id_vars=['Approach'],\n",
    "                                                       value_vars=['Precision Class 0', 'Recall Class 0'],\n",
    "                                                       var_name='Metric', value_name='Value')\n",
    "\n",
    "    sns.barplot(data=df_melted_prec_recall, x='Approach', y='Value', hue='Metric', palette=[\"#440154\", \"#FDE725\"])\n",
    "\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.title(\"Top & Bottom 10 Models: Precision & Recall for Class 0 (Sorted)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def scale_df(X_train, X_test, columns):\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[columns])\n",
    "    \n",
    "    X_train_scaled[columns] = scaler.transform(X_train[columns])\n",
    "    X_test_scaled[columns] = scaler.transform(X_test[columns])\n",
    "    \n",
    "    return X_train_scaled[columns], X_test_scaled[columns], columns\n",
    "\n",
    "def scale_and_pca_with_elbow(X_train, X_test, columns, explained_variance_threshold=0.95):\n",
    "    X_train_scaled, X_test_scaled, columns = scale_df(X_train, X_test, columns)\n",
    "    \n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(X_train_scaled[columns])\n",
    "\n",
    "    elbow = 1\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    if cumulative_explained_variance[elbow - 1] < explained_variance_threshold:\n",
    "        elbow = np.argmax(cumulative_explained_variance >= explained_variance_threshold) + 1\n",
    "\n",
    "    pca_cols = [f\"pca{i}\"for i in range(1, elbow+1)]\n",
    "\n",
    "    pca = PCA(n_components=elbow)\n",
    "    pca.fit(X_train_scaled[columns])\n",
    "    X_train_pca_data = pca.transform(X_train_scaled[columns])\n",
    "    X_test_pca_data = pca.transform(X_test_scaled[columns])\n",
    "    \n",
    "    for i, pca_col in enumerate(pca_cols):\n",
    "        X_train_scaled[pca_col] = X_train_pca_data[:, i]\n",
    "        X_test_scaled[pca_col] = X_test_pca_data[:, i]\n",
    "\n",
    "    return X_train_scaled[pca_cols],  X_test_scaled[pca_cols], pca_cols\n",
    "\n",
    "def filter_low_representation_columns(X_train, X_test, columns, threshold=0.01):\n",
    "    X_train_copy = X_train.copy()\n",
    "    binary_cols = X_train_copy[columns].select_dtypes(include=['uint8', 'int64']).columns\n",
    "    new_cols = []\n",
    "    for col in binary_cols:\n",
    "        ones_percentage = X_train_copy[col].mean()\n",
    "        if ones_percentage >= threshold:\n",
    "            new_cols.append(col)\n",
    "    \n",
    "    dropped_cols = set(binary_cols) - set(new_cols)\n",
    "    new_columns = list(set(columns) - dropped_cols)\n",
    "    \n",
    "    # print(f\"Dropped a total of: {len(columns) - len(new_columns)}\") \n",
    "    return X_train_copy[new_columns], X_test[new_columns], new_columns\n",
    "\n",
    "def smooth_target_encoding(X_train, X_test, columns, smoothing=1):\n",
    "    encoded_X_train = X_train.copy()\n",
    "    encoded_X_test = X_test.copy()\n",
    "    global_mean = encoded_X_train[y_column].mean()\n",
    "    target_cols = ['Specialization in study', 'State (Location)']\n",
    "    keywords = [f'dummy_{col}' for col in target_cols]\n",
    "    filtered_columns = [col for col in columns if not any(keyword in col for keyword in keywords)]\n",
    "    \n",
    "    for col in target_cols:\n",
    "        means = encoded_X_train.groupby(col)[y_column].mean()\n",
    "        counts = encoded_X_train.groupby(col)[y_column].count()\n",
    "\n",
    "        smoothed_values = {\n",
    "            value: (means[value] * counts[value] + global_mean * smoothing) / (counts[value] + smoothing)\n",
    "            for value in means.index\n",
    "        }\n",
    "    \n",
    "        # Apply smoothing formula\n",
    "        encoded_X_train[col] = encoded_X_train[col].map(smoothed_values)\n",
    "        encoded_X_test[col] = encoded_X_test[col].map(smoothed_values).fillna(global_mean)\n",
    "        \n",
    "        \n",
    "    final_cols = filtered_columns + target_cols\n",
    "        \n",
    "    return encoded_X_train[final_cols], encoded_X_test[final_cols], final_cols\n",
    "\n",
    "def combined1(X_train, X_test, columns):\n",
    "    X_train_copy, X_test_copy, new_cols = smooth_target_encoding(X_train, X_test, columns)\n",
    "    X_train_copy, X_test_copy, new_cols = scale_df(X_train_copy, X_test_copy, new_cols)\n",
    "    return X_train_copy, X_test_copy, new_cols\n",
    "\n",
    "def combined2(X_train, X_test, columns):\n",
    "    X_train_copy, X_test_copy, new_cols = smooth_target_encoding(X_train, X_test, columns)\n",
    "    X_train_copy, X_test_copy, new_cols = scale_and_pca_with_elbow(X_train_copy, X_test_copy, new_cols)\n",
    "    return  X_train_copy, X_test_copy, new_cols\n",
    "\n",
    "def combined3(X_train, X_test, columns):\n",
    "    X_train_copy, X_test_copy, new_cols = smooth_target_encoding(X_train, X_test, columns)\n",
    "    X_train_copy, X_test_copy, new_cols = scale_and_pca_with_elbow(X_train_copy, X_test_copy, new_cols)\n",
    "    X_train_copy, X_test_copy, new_cols = filter_low_representation_columns(X_train_copy, X_test_copy, new_cols)\n",
    "    return X_train_copy, X_test_copy, new_cols\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'DecisionTree': DecisionTreeClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100),\n",
    "    'SVC': SVC(kernel='rbf', C=1.0),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'KNeighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100),\n",
    "    'MLPClassifier': MLPClassifier(hidden_layer_sizes=(1000,), max_iter=10000),\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    '': lambda X_train, X_test, columns: (X_train[columns], X_test[columns], columns),\n",
    "    'scale': scale_df, \n",
    "    'pca': scale_and_pca_with_elbow,\n",
    "    'filter_cols': filter_low_representation_columns,\n",
    "    'smooth_target_encoding': smooth_target_encoding,\n",
    "    'ste + scale': combined1,\n",
    "    'ste + scale + pca': combined2,\n",
    "    'ste + scale + pca + filter': combined3\n",
    "    \n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = evaluate_models(df, models, classification_columns, y_column, pipelines=pipelines, k=5)\n",
    "results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results.to_csv('model_tests_results.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_results_df = pd.read_csv('model_tests_results.csv')\n",
    "model_results_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_random_forest_with_shap(df, classification_columns, smooth_target_encoding, y_column=y_column, test_size=0.2, random_state=42)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_model_results(model_results_df)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
